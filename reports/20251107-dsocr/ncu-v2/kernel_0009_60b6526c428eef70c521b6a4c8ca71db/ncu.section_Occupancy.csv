"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description","Estimated Speedup Type","Estimated Speedup"
"0","1745130","python3.12","127.0.0.1","fmha_cutlassF_bf16_aligned_64x64_rf_sm80(AttentionKernel<bfloat16_t, Sm80, 1, 64, 64, 64, 1, 1>::Params)","1","7","(32, 4, 1)","(4, 12, 54)","0","12.0","Occupancy","Max Active Clusters","cluster","0",
"0","1745130","python3.12","127.0.0.1","fmha_cutlassF_bf16_aligned_64x64_rf_sm80(AttentionKernel<bfloat16_t, Sm80, 1, 64, 64, 64, 1, 1>::Params)","1","7","(32, 4, 1)","(4, 12, 54)","0","12.0","Occupancy","Max Cluster Size","block","8",
"0","1745130","python3.12","127.0.0.1","fmha_cutlassF_bf16_aligned_64x64_rf_sm80(AttentionKernel<bfloat16_t, Sm80, 1, 64, 64, 64, 1, 1>::Params)","1","7","(32, 4, 1)","(4, 12, 54)","0","12.0","Occupancy","Overall GPU Occupancy","%","0",
"0","1745130","python3.12","127.0.0.1","fmha_cutlassF_bf16_aligned_64x64_rf_sm80(AttentionKernel<bfloat16_t, Sm80, 1, 64, 64, 64, 1, 1>::Params)","1","7","(32, 4, 1)","(4, 12, 54)","0","12.0","Occupancy","Cluster Occupancy","%","0",
"0","1745130","python3.12","127.0.0.1","fmha_cutlassF_bf16_aligned_64x64_rf_sm80(AttentionKernel<bfloat16_t, Sm80, 1, 64, 64, 64, 1, 1>::Params)","1","7","(32, 4, 1)","(4, 12, 54)","0","12.0","Occupancy","Block Limit Barriers","block","24",
"0","1745130","python3.12","127.0.0.1","fmha_cutlassF_bf16_aligned_64x64_rf_sm80(AttentionKernel<bfloat16_t, Sm80, 1, 64, 64, 64, 1, 1>::Params)","1","7","(32, 4, 1)","(4, 12, 54)","0","12.0","Occupancy","Block Limit SM","block","24",
"0","1745130","python3.12","127.0.0.1","fmha_cutlassF_bf16_aligned_64x64_rf_sm80(AttentionKernel<bfloat16_t, Sm80, 1, 64, 64, 64, 1, 1>::Params)","1","7","(32, 4, 1)","(4, 12, 54)","0","12.0","Occupancy","Block Limit Registers","block","4",
"0","1745130","python3.12","127.0.0.1","fmha_cutlassF_bf16_aligned_64x64_rf_sm80(AttentionKernel<bfloat16_t, Sm80, 1, 64, 64, 64, 1, 1>::Params)","1","7","(32, 4, 1)","(4, 12, 54)","0","12.0","Occupancy","Block Limit Shared Mem","block","5",
"0","1745130","python3.12","127.0.0.1","fmha_cutlassF_bf16_aligned_64x64_rf_sm80(AttentionKernel<bfloat16_t, Sm80, 1, 64, 64, 64, 1, 1>::Params)","1","7","(32, 4, 1)","(4, 12, 54)","0","12.0","Occupancy","Block Limit Warps","block","12",
"0","1745130","python3.12","127.0.0.1","fmha_cutlassF_bf16_aligned_64x64_rf_sm80(AttentionKernel<bfloat16_t, Sm80, 1, 64, 64, 64, 1, 1>::Params)","1","7","(32, 4, 1)","(4, 12, 54)","0","12.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"0","1745130","python3.12","127.0.0.1","fmha_cutlassF_bf16_aligned_64x64_rf_sm80(AttentionKernel<bfloat16_t, Sm80, 1, 64, 64, 64, 1, 1>::Params)","1","7","(32, 4, 1)","(4, 12, 54)","0","12.0","Occupancy","Theoretical Occupancy","%","33.33",
"0","1745130","python3.12","127.0.0.1","fmha_cutlassF_bf16_aligned_64x64_rf_sm80(AttentionKernel<bfloat16_t, Sm80, 1, 64, 64, 64, 1, 1>::Params)","1","7","(32, 4, 1)","(4, 12, 54)","0","12.0","Occupancy","Achieved Occupancy","%","30.81",
"0","1745130","python3.12","127.0.0.1","fmha_cutlassF_bf16_aligned_64x64_rf_sm80(AttentionKernel<bfloat16_t, Sm80, 1, 64, 64, 64, 1, 1>::Params)","1","7","(32, 4, 1)","(4, 12, 54)","0","12.0","Occupancy","Achieved Active Warps Per SM","warp","14.79",
"0","1745130","python3.12","127.0.0.1","fmha_cutlassF_bf16_aligned_64x64_rf_sm80(AttentionKernel<bfloat16_t, Sm80, 1, 64, 64, 64, 1, 1>::Params)","1","7","(32, 4, 1)","(4, 12, 54)","0","12.0","Occupancy","","","","TheoreticalOccupancy","OPT","The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of required registers.","global","61.96"
