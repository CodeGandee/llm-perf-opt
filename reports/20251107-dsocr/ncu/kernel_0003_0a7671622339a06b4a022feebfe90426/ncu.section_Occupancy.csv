"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description","Estimated Speedup Type","Estimated Speedup"
"0","1703369","python3.12","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_bf16_s161616gemm_bf16_32x32_128x2_tn_align8>(Params)","1","7","(128, 1, 1)","(72, 4, 1)","0","12.0","Occupancy","Max Active Clusters","cluster","0",
"0","1703369","python3.12","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_bf16_s161616gemm_bf16_32x32_128x2_tn_align8>(Params)","1","7","(128, 1, 1)","(72, 4, 1)","0","12.0","Occupancy","Max Cluster Size","block","8",
"0","1703369","python3.12","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_bf16_s161616gemm_bf16_32x32_128x2_tn_align8>(Params)","1","7","(128, 1, 1)","(72, 4, 1)","0","12.0","Occupancy","Overall GPU Occupancy","%","0",
"0","1703369","python3.12","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_bf16_s161616gemm_bf16_32x32_128x2_tn_align8>(Params)","1","7","(128, 1, 1)","(72, 4, 1)","0","12.0","Occupancy","Cluster Occupancy","%","0",
"0","1703369","python3.12","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_bf16_s161616gemm_bf16_32x32_128x2_tn_align8>(Params)","1","7","(128, 1, 1)","(72, 4, 1)","0","12.0","Occupancy","Block Limit Barriers","block","24",
"0","1703369","python3.12","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_bf16_s161616gemm_bf16_32x32_128x2_tn_align8>(Params)","1","7","(128, 1, 1)","(72, 4, 1)","0","12.0","Occupancy","Block Limit SM","block","24",
"0","1703369","python3.12","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_bf16_s161616gemm_bf16_32x32_128x2_tn_align8>(Params)","1","7","(128, 1, 1)","(72, 4, 1)","0","12.0","Occupancy","Block Limit Registers","block","5",
"0","1703369","python3.12","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_bf16_s161616gemm_bf16_32x32_128x2_tn_align8>(Params)","1","7","(128, 1, 1)","(72, 4, 1)","0","12.0","Occupancy","Block Limit Shared Mem","block","2",
"0","1703369","python3.12","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_bf16_s161616gemm_bf16_32x32_128x2_tn_align8>(Params)","1","7","(128, 1, 1)","(72, 4, 1)","0","12.0","Occupancy","Block Limit Warps","block","12",
"0","1703369","python3.12","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_bf16_s161616gemm_bf16_32x32_128x2_tn_align8>(Params)","1","7","(128, 1, 1)","(72, 4, 1)","0","12.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"0","1703369","python3.12","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_bf16_s161616gemm_bf16_32x32_128x2_tn_align8>(Params)","1","7","(128, 1, 1)","(72, 4, 1)","0","12.0","Occupancy","Theoretical Occupancy","%","16.67",
"0","1703369","python3.12","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_bf16_s161616gemm_bf16_32x32_128x2_tn_align8>(Params)","1","7","(128, 1, 1)","(72, 4, 1)","0","12.0","Occupancy","Achieved Occupancy","%","13.73",
"0","1703369","python3.12","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_bf16_s161616gemm_bf16_32x32_128x2_tn_align8>(Params)","1","7","(128, 1, 1)","(72, 4, 1)","0","12.0","Occupancy","Achieved Active Warps Per SM","warp","6.59",
"0","1703369","python3.12","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_bf16_s161616gemm_bf16_32x32_128x2_tn_align8>(Params)","1","7","(128, 1, 1)","(72, 4, 1)","0","12.0","Occupancy","","","","TheoreticalOccupancy","OPT","The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of shared memory.","global","52.04"
