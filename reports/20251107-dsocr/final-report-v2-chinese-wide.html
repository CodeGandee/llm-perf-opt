<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang xml:lang>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>final-report-v2-chinese</title>
  <style>

html {
color: #1a1a1a;
background-color: #fdfdfd;
}
body {
margin: 0 auto;
max-width: 36em;
padding-left: 50px;
padding-right: 50px;
padding-top: 50px;
padding-bottom: 50px;
hyphens: auto;
overflow-wrap: break-word;
text-rendering: optimizeLegibility;
font-kerning: normal;
}
@media (max-width: 600px) {
body {
font-size: 0.9em;
padding: 12px;
}
h1 {
font-size: 1.8em;
}
}
@media print {
html {
background-color: white;
}
body {
background-color: transparent;
color: black;
font-size: 12pt;
}
p, h2, h3 {
orphans: 3;
widows: 3;
}
h2, h3, h4 {
page-break-after: avoid;
}
}
p {
margin: 1em 0;
}
a {
color: #1a1a1a;
}
a:visited {
color: #1a1a1a;
}
img {
max-width: 100%;
}
svg {
height: auto;
max-width: 100%;
}
h1, h2, h3, h4, h5, h6 {
margin-top: 1.4em;
}
h5, h6 {
font-size: 1em;
font-style: italic;
}
h6 {
font-weight: normal;
}
ol, ul {
padding-left: 1.7em;
margin-top: 1em;
}
li > ol, li > ul {
margin-top: 0;
}
blockquote {
margin: 1em 0 1em 1.7em;
padding-left: 1em;
border-left: 2px solid #e6e6e6;
color: #606060;
}
code {
font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
font-size: 85%;
margin: 0;
hyphens: manual;
}
pre {
margin: 1em 0;
overflow: auto;
}
pre code {
padding: 0;
overflow: visible;
overflow-wrap: normal;
}
.sourceCode {
background-color: transparent;
overflow: visible;
}
hr {
border: none;
border-top: 1px solid #1a1a1a;
height: 1px;
margin: 1em 0;
}
table {
margin: 1em 0;
border-collapse: collapse;
width: 100%;
overflow-x: auto;
display: block;
font-variant-numeric: lining-nums tabular-nums;
}
table caption {
margin-bottom: 0.75em;
}
tbody {
margin-top: 0.5em;
border-top: 1px solid #1a1a1a;
border-bottom: 1px solid #1a1a1a;
}
th {
border-top: 1px solid #1a1a1a;
padding: 0.25em 0.5em 0.25em 0.5em;
}
td {
padding: 0.125em 0.5em 0.25em 0.5em;
}
header {
margin-bottom: 4em;
text-align: center;
}
#TOC li {
list-style: none;
}
#TOC ul {
padding-left: 1.3em;
}
#TOC > ul {
padding-left: 0;
}
#TOC a:not(:hover) {
text-decoration: none;
}
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}

ul.task-list[class]{list-style: none;}
ul.task-list li input[type="checkbox"] {
font-size: inherit;
width: 0.8em;
margin: 0 0.8em 0.2em -1.6em;
vertical-align: middle;
}
.display.math{display: block; text-align: center; margin: 0.5rem auto;}
</style>
</head>
<body>
<h1 id="deepseek-ocr-内核级性能分析报告v2">DeepSeek-OCR
内核级性能分析报告（V2）</h1>
<h2 id="概览">概览</h2>
<p>这是第 2 版（V2）报告，覆盖全部 20 个 Top 内核的完整剖析数据。V1 中有
2 个内核（kernel_0009 与 kernel_0016）缺失数据；在 V2
中已全部成功采集并完成分析。</p>
<p>本报告旨在基于 NVIDIA Nsight Systems（nsys）与 Nsight
Compute（ncu）对 DeepSeek-OCR
模型进行内核级性能剖析，识别推理阶段的性能特征与潜在瓶颈，为面向推理的
NPU 设计提供参考。</p>
<p>本报告包含： - DeepSeek-OCR 推理阶段的 Top 内核 - 来自 Nsight
Compute（NCU）的 20 个内核完整性能指标 - 内核执行指标的直方图 -
全量屋脊线（Roofline）分析 -
按类型（计算受限、内存受限、均衡）的内核分类 -
剖析环境与方法的实现细节</p>
<p>V2 的主要改进： - 100% 内核覆盖：20/20 全部成功剖析（V1 为 18/20） -
kernel_0009 现被归类为“均衡”（V1 为未知） - kernel_0016
现被归类为“计算受限”（V1 为未知） - 屋脊线分析更完整，基于全量数据集</p>
<h2 id="实验">实验</h2>
<h3 id="环境配置">环境配置</h3>
<p>硬件： - GPU：NVIDIA GeForce RTX 5090（Blackwell 架构，sm_120） -
计算能力：12.0</p>
<p>软件： - CUDA：12.8 - PyTorch：2.10.0.dev20251102+cu128 -
Transformers：4.46.3 - 剖析工具：NVIDIA Nsight Systems、NVIDIA Nsight
Compute</p>
<p>模型配置： - 模型：DeepSeek-OCR - 数值精度：BF16（bfloat16） -
推理模式：自回归生成（Autoregressive） - 最大新生成 Token：64 -
温度：0.0（贪心解码） - 上下文长度模式：Auto - 不重复 N-gram
大小：20</p>
<p>数据集： - 数据集：OmniDocBench - 样本：20 张图像（dev-20 子集） -
预处理：Base size 1024、image size 640、启用裁剪模式</p>
<p>剖析配置： - 阶段 1：PyTorch Profiler（算子级分析；对阶段 2
的工作负载关闭） - 阶段 2：Nsight Systems（CUDA 时间线，使用 NVTX
对“decode”阶段门控） - Nsight
Compute：Roofline、SpeedOfLight、MemoryWorkloadAnalysis、Occupancy
等章节 - 内核选择：基于 Nsys 总时间 Top 20（按解码阶段累计执行时间）</p>
<h3 id="按总时间排序的内核top-15">按总时间排序的内核（Top 15）</h3>
<p>下表展示解码阶段（decode）累计执行时间排名前 15 的内核（由 Nsight
Systems 度量）。按来源库进行归类，并给出基于功能的易读名称。</p>
<table style="width:100%;">
<colgroup>
<col style="width: 33%" />
<col style="width: 14%" />
<col style="width: 29%" />
<col style="width: 22%" />
</colgroup>
<thead>
<tr>
<th>时间占比</th>
<th>库</th>
<th>内核名</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>34.4%</td>
<td>cuBLAS</td>
<td>GEMV-1 (BF16, template=7)</td>
<td>矩阵-向量乘，因自回归解码而占比最高</td>
</tr>
<tr>
<td>14.8%</td>
<td>cuBLAS</td>
<td>GEMV-2 (BF16, template=6)</td>
<td>矩阵-向量乘，替代的分块/铺片策略</td>
</tr>
<tr>
<td>6.0%</td>
<td>PyTorch ATen</td>
<td>Direct Copy (float)</td>
<td>中间张量的内存拷贝/类型转换</td>
</tr>
<tr>
<td>4.2%</td>
<td>PyTorch ATen</td>
<td>Elementwise Multiply (BF16, vec)</td>
<td>向量化点乘（如注意力掩码）</td>
</tr>
<tr>
<td>3.9%</td>
<td>PyTorch ATen</td>
<td>SiLU Activation (BF16, vec)</td>
<td>FFN 中向量化 SiLU（Swish）激活</td>
</tr>
<tr>
<td>2.9%</td>
<td>PyTorch ATen</td>
<td>Elementwise Multiply (BF16)</td>
<td>非向量化点乘</td>
</tr>
<tr>
<td>2.8%</td>
<td>PyTorch ATen</td>
<td>Cat Batched Copy (vec, 128-tile)</td>
<td>多头输出拼接拷贝</td>
</tr>
<tr>
<td>2.7%</td>
<td>PyTorch ATen</td>
<td>Copy/Cast (BF16, vec)</td>
<td>BF16 专用向量化拷贝</td>
</tr>
<tr>
<td>2.4%</td>
<td>FlashAttention</td>
<td>Flash Forward Split-KV (BF16)</td>
<td>面向 IO 的分裂 K/V 融合注意力</td>
</tr>
<tr>
<td>2.0%</td>
<td>PyTorch ATen</td>
<td>Elementwise Add (BF16, vec)</td>
<td>向量化加法（残差）</td>
</tr>
<tr>
<td>1.8%</td>
<td>PyTorch ATen</td>
<td>Cat Batched Copy (vec, 64-tile)</td>
<td>更小分块的拼接拷贝</td>
</tr>
<tr>
<td>1.8%</td>
<td>PyTorch ATen</td>
<td>Elementwise Multiply (float)</td>
<td>FP32 点乘</td>
</tr>
<tr>
<td>1.6%</td>
<td>PyTorch ATen</td>
<td>Mean Reduction (float)</td>
<td>归一化相关归约</td>
</tr>
<tr>
<td>1.5%</td>
<td>PyTorch ATen</td>
<td>Elementwise Neg (BF16)</td>
<td>取负运算</td>
</tr>
<tr>
<td>1.4%</td>
<td>FlashAttention</td>
<td>Flash Split-KV Combine</td>
<td>合并分裂 K/V 的输出</td>
</tr>
</tbody>
</table>
<p>关键观察： - GEMV 主导（49.2%）：两种 cuBLAS GEMV
变体占据近一半的执行时间，符合 batch=1 的自回归解码特征（GEMV 多于
GEMM）。 - ATen
点算子占比高（21.7%）：大量时间花在点算子内核上，存在融合空间。 -
FlashAttention（3.8%）：尽管高度优化，注意力仍占据一定比例。 -
内存操作（8.7%）：直接拷贝与类型转换存在可优化的布局与数据通路空间。</p>
<h3 id="内核分类">内核分类</h3>
<p>对 Top 20 内核全部进行了 NCU 剖析，成功率 100%（V1 为
18/20）。基于屋脊线分析进行分类：</p>
<table>
<thead>
<tr>
<th>分类</th>
<th>数量</th>
<th>占比</th>
</tr>
</thead>
<tbody>
<tr>
<td>内存受限</td>
<td>9</td>
<td>45.0%</td>
</tr>
<tr>
<td>均衡</td>
<td>7</td>
<td>35.0%</td>
</tr>
<tr>
<td>计算受限</td>
<td>4</td>
<td>20.0%</td>
</tr>
<tr>
<td>未知</td>
<td>0</td>
<td>0% ✓</td>
</tr>
</tbody>
</table>
<p>按分类的性能特征：</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>计算受限</th>
<th>内存受限</th>
<th>均衡</th>
<th>总体均值</th>
</tr>
</thead>
<tbody>
<tr>
<td>SM 吞吐</td>
<td>36.21%</td>
<td>9.38%</td>
<td>12.25%</td>
<td>15.75%</td>
</tr>
<tr>
<td>DRAM 吞吐</td>
<td>9.89%</td>
<td>34.23%</td>
<td>12.62%</td>
<td>21.80%</td>
</tr>
<tr>
<td>内存吞吐</td>
<td>32.23%</td>
<td>41.20%</td>
<td>27.52%</td>
<td>34.62%</td>
</tr>
<tr>
<td>实际占用率</td>
<td>31.51%</td>
<td>43.22%</td>
<td>39.32%</td>
<td>39.51%</td>
</tr>
<tr>
<td>L1 命中率</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>8.96%</td>
</tr>
<tr>
<td>L2 命中率</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>39.95%</td>
</tr>
<tr>
<td>平均时长</td>
<td>137.44 μs</td>
<td>9.67 μs</td>
<td>39.75 μs</td>
<td>45.75 μs</td>
</tr>
</tbody>
</table>
<p>要点： 1. 完整数据集（V2 改进）：20 个内核全部完成分类，消除了 V1
的不确定性。 2. SM 吞吐提升（15.75%）：较
V1（10.35%）更高，完整数据带来更准确的利用率评估。 3.
内存吞吐提升（34.62%，V1 为 27.35%）：新增内核提升了平均内存利用。 4.
占用率更高（39.51%，V1 为
30.13%）：完整数据显著提升评估值，但总体仍偏低。 5.
内存受限占比最高（45%）：近半数内核受限于内存带宽。 6.
计算受限数量增加：4 个（20%），较 V1 的 3
个（16.7%）有所增加；kernel_0016 加入该类。 7.
时长分布差异大：计算受限内核单次更慢（均值 137.44
μs）而内存受限更短（9.67 μs），表明计算型内核更复杂。 8. L2
缓存命中率下降：由 V1 的 53.48% 降至 V2 的
39.95%，暗示新增内核缓存行为更差。</p>
<p>新增分类（V1 → V2）： - kernel_0009：未知 → 均衡（fmha_cutlassF
memory-efficient attention） - kernel_0016：未知 → 计算受限（CUTLASS
GEMM 内核）</p>
<h3 id="按单次时长排序ncu">按单次时长排序（NCU）</h3>
<p>下列内核在 V2 中具有最长的单次执行时长：</p>
<table>
<thead>
<tr>
<th>时长（μs）</th>
<th>分类</th>
<th>内核</th>
</tr>
</thead>
<tbody>
<tr>
<td>404.70</td>
<td>计算受限</td>
<td>CUTLASS GEMM (128×256 tile, BF16, ReLU)</td>
</tr>
<tr>
<td>160.74</td>
<td>均衡</td>
<td>Memory-Efficient Attention (CUTLASS BF16, 64×64)</td>
</tr>
<tr>
<td>111.81</td>
<td>计算受限</td>
<td>CUTLASS GEMM (256×128 tile, BF16, ReLU)</td>
</tr>
<tr>
<td>94.91</td>
<td>均衡</td>
<td>ATen Elementwise Add (BF16, 6144 blocks)</td>
</tr>
<tr>
<td>23.42</td>
<td>计算受限</td>
<td>CUTLASS GEMM (32×32 tile, BF16, WMMA)</td>
</tr>
<tr>
<td>18.34</td>
<td>内存受限</td>
<td>cuBLAS GEMV (BF16, template=6)</td>
</tr>
<tr>
<td>18.08</td>
<td>内存受限</td>
<td>ATen Direct Copy (float, 12288 blocks)</td>
</tr>
<tr>
<td>10.34</td>
<td>内存受限</td>
<td>FlashAttention Split-KV Forward</td>
</tr>
<tr>
<td>9.82</td>
<td>计算受限</td>
<td>ATen Cat Batched Copy (512-thread blocks)</td>
</tr>
<tr>
<td>7.97</td>
<td>内存受限</td>
<td>ATen Mean Reduction (float, 512 threads)</td>
</tr>
</tbody>
</table>
<p>分析： - 最长内核显著增加（404.70 μs，V1 为 165.09 μs）：提升 2.45
倍，可能源于执行形态差异或测量差异。 - 记忆高效注意力出现（160.74
μs）：即 V1 未知的 kernel_0009，现归为“均衡”，为第 2
慢内核，说明注意力计算贡献显著。 - CUTLASS GEMM 占优：前 4 个中的 3
个为大分块的计算受限 GEMM。 - “均衡”并不便宜：如
kernel_0009（注意力，160.74 μs）与 elementwise add（94.91
μs），同时低效使用计算与内存。 - 内存受限内核更“短平快”：均值 9.67
μs，符合快速内存操作特征。 - FlashAttention 仅 10.34
μs：作为复杂的融合注意力，显著快于 160.74 μs 的记忆高效注意力变体。</p>
<h3 id="内核执行指标的直方图">内核执行指标的直方图</h3>
<p>以下直方图展示了所有被剖析内核在关键性能指标上的分布，用于发现共性与离群点。</p>
<h4 id="计算与内存吞吐">计算与内存吞吐</h4>
<p>SM 吞吐（峰值百分比）</p>
<figure>
<img role="img" aria-label="SM Throughput Distribution" src="ncu-v2/analysis/histograms/sm_throughput_pct.png" alt="SM Throughput Distribution" />
<figcaption aria-hidden="true">SM Throughput Distribution</figcaption>
</figure>
<p>大多数内核的 SM
利用率很低（&lt;15%），验证了整体计算资源利用不足。</p>
<p>DRAM 吞吐（峰值百分比）</p>
<figure>
<img role="img" aria-label="DRAM Throughput Distribution" src="ncu-v2/analysis/histograms/dram_throughput_pct.png" alt="DRAM Throughput Distribution" />
<figcaption aria-hidden="true">DRAM Throughput Distribution</figcaption>
</figure>
<p>DRAM
吞吐呈双峰分布：大量内核处于很低（&lt;10%）或中等（20-30%）区域，显示出内存受限与计算受限两类群体。</p>
<p>内存吞吐（峰值百分比）</p>
<figure>
<img role="img" aria-label="Memory Throughput Distribution" src="ncu-v2/analysis/histograms/mem_throughput_pct.png" alt="Memory Throughput Distribution" />
<figcaption aria-hidden="true">Memory Throughput
Distribution</figcaption>
</figure>
<p>综合内存吞吐指标呈相似的双峰模式，内存受限内核可达到 40-90%
的利用率。</p>
<p>内存吞吐（GB/s）</p>
<figure>
<img role="img" aria-label="Memory Throughput Absolute" src="ncu-v2/analysis/histograms/memory_throughput_gbs.png" alt="Memory Throughput Absolute" />
<figcaption aria-hidden="true">Memory Throughput Absolute</figcaption>
</figure>
<p>绝对带宽显示多数内核在 5-400 GB/s 范围内运行，远低于 RTX 5090
的理论峰值。</p>
<h4 id="缓存性能">缓存性能</h4>
<p>L1 纹理缓存命中率</p>
<figure>
<img role="img" aria-label="L1 Hit Rate Distribution" src="ncu-v2/analysis/histograms/l1tex_hit_rate_pct.png" alt="L1 Hit Rate Distribution" />
<figcaption aria-hidden="true">L1 Hit Rate Distribution</figcaption>
</figure>
<p>L1 命中率极低，多数内核仅 0-10% 命中，提示工作集超出 L1
容量或空间局部性较差。</p>
<p>L1 纹理缓存吞吐（峰值百分比）</p>
<figure>
<img role="img" aria-label="L1 Throughput Distribution" src="ncu-v2/analysis/histograms/l1tex_throughput_pct.png" alt="L1 Throughput Distribution" />
<figcaption aria-hidden="true">L1 Throughput Distribution</figcaption>
</figure>
<p>L1 吞吐分布较宽（2-50%），表明不同内核的缓存访问模式差异较大。</p>
<p>L2 缓存命中率</p>
<figure>
<img role="img" aria-label="L2 Hit Rate Distribution" src="ncu-v2/analysis/histograms/l2_hit_rate_pct.png" alt="L2 Hit Rate Distribution" />
<figcaption aria-hidden="true">L2 Hit Rate Distribution</figcaption>
</figure>
<p>L2 命中率明显好于 L1，聚集在 15-90%，说明工作集有部分可在 L2
中复用。</p>
<p>L2 缓存吞吐（峰值百分比）</p>
<figure>
<img role="img" aria-label="L2 Throughput Distribution" src="ncu-v2/analysis/histograms/l2_throughput_pct.png" alt="L2 Throughput Distribution" />
<figcaption aria-hidden="true">L2 Throughput Distribution</figcaption>
</figure>
<p>L2 吞吐多数位于 1-30%，存在 80-90% 的离群点，表征重度 L2 流量。</p>
<h4 id="占用率指标">占用率指标</h4>
<p>实际占用率（Achieved Occupancy）</p>
<figure>
<img role="img" aria-label="Achieved Occupancy Distribution" src="ncu-v2/analysis/histograms/achieved_occupancy_pct.png" alt="Achieved Occupancy Distribution" />
<figcaption aria-hidden="true">Achieved Occupancy
Distribution</figcaption>
</figure>
<p>实际占用率分布很散，从接近 0% 到 90%，大量内核位于
7-40%，提示并行度不足。</p>
<p>理论占用率（Theoretical Occupancy）</p>
<figure>
<img role="img" aria-label="Theoretical Occupancy Distribution" src="ncu-v2/analysis/histograms/theoretical_occupancy_pct.png" alt="Theoretical Occupancy Distribution" />
<figcaption aria-hidden="true">Theoretical Occupancy
Distribution</figcaption>
</figure>
<p>理论上多数内核可达到 80-100%
占用，但实际远低，说明运行时瓶颈（内存延迟、同步）阻碍了利用率提升。</p>
<h4 id="内核时长与内存活动">内核时长与内存活动</h4>
<p>内核时长（Duration）</p>
<figure>
<img role="img" aria-label="Duration Distribution" src="ncu-v2/analysis/histograms/duration_us.png" alt="Duration Distribution" />
<figcaption aria-hidden="true">Duration Distribution</figcaption>
</figure>
<p>内核时长分布在 2-170 μs，多数内核很短（&lt;10
μs），其启动开销相对计算时间可能较高。</p>
<p>内存忙碌百分比（Memory Busy %）</p>
<figure>
<img role="img" aria-label="Memory Busy Distribution" src="ncu-v2/analysis/histograms/mem_busy_pct.png" alt="Memory Busy Distribution" />
<figcaption aria-hidden="true">Memory Busy Distribution</figcaption>
</figure>
<p>内存子系统忙碌时间跨度较大，内存受限内核可让内存保持 20-90%
的忙碌度。</p>
<p>最大带宽利用率（Max Bandwidth %）</p>
<figure>
<img role="img" aria-label="Max Bandwidth Distribution" src="ncu-v2/analysis/histograms/max_bandwidth_pct.png" alt="Max Bandwidth Distribution" />
<figcaption aria-hidden="true">Max Bandwidth Distribution</figcaption>
</figure>
<p>跨全部内存类型的峰值带宽利用率大多仅 5-50% 的水平。</p>
<p>上述直方图揭示了被剖析内核在关键性能指标上的分布特征，便于识别共性与离群点。</p>
<h3 id="屋脊线roofline分析">屋脊线（Roofline）分析</h3>
<p>屋脊线分析以“算术强度（每字节 FLOPs）-
实际性能（FLOP/s）”散点，刻画内核相对硬件上限的性能表现。</p>
<h4 id="归一化屋脊线">归一化屋脊线</h4>
<figure>
<img role="img" aria-label="Roofline Scatter - Normalized" src="ncu-v2/analysis/roofline_scatter_normalized.png" alt="Roofline Scatter - Normalized" />
<figcaption aria-hidden="true">Roofline Scatter -
Normalized</figcaption>
</figure>
<p>要点： - 多数内核位于低算术强度区域（&lt;100
FLOPs/byte），表现为内存受限； - 很少有内核接近“计算屋脊”（右上斜线）；
- 实际性能与屋脊线的差距显示出可观的优化空间。</p>
<h4 id="物理屋脊线">物理屋脊线</h4>
<figure>
<img role="img" aria-label="Roofline Scatter - Physical" src="ncu-v2/analysis/roofline_scatter_physical.png" alt="Roofline Scatter - Physical" />
<figcaption aria-hidden="true">Roofline Scatter - Physical</figcaption>
</figure>
<p>结合 RTX 5090 的真实计算/带宽上限： - 左侧的内存受限内核主要受 DRAM
带宽约束，尽管算术强度不同，FLOP/s 多低于 500 GFLOP/s； -
右侧的计算受限内核 FLOP/s 较高，但仍明显低于张量核心峰值吞吐； -
该工作负载的内存/计算受限分界约在 50-100 FLOPs/byte。</p>
<p>每个内核的单独屋脊线图（归一化与物理）均可在
<code>ncu-v2/analysis/roofline_per_kernel/</code> 中查看。</p>
<p>屋脊线解读： - 内存受限（约
45%）：低算术强度区域，受限于带宽，宜做访存/布局/融合优化； -
计算受限（约 20%）：虽处高算术强度区，但 SM 吞吐仅
~34%，提示张量核利用率或并行度不足； - 均衡（约
35%）：处于过渡区，计算与内存利用均偏低，整体效率不高。</p>
<p>总体上，多数内核远未达到内存或计算屋脊，存在显著优化空间；大多数内核实际性能低于理论上限的
50%。</p>
<h2 id="结论">结论</h2>
<h3 id="主要发现">主要发现</h3>
<ol type="1">
<li>覆盖完整（V2 达成）：Top 20 全部完成剖析，相比 V1 的 90% 覆盖提升为
100%；此前未知的 kernel_0009 与 kernel_0016 已全部刻画。</li>
<li>GEMV 瓶颈（49.2% 时间）：自回归解码以 GEMV
为主（batch=1），天然更偏内存受限，计算利用率在现代 GPU 上不高。</li>
<li>硬件利用率虽有提升但仍偏低：
<ul>
<li>SM 吞吐 15.75%（V1 为 10.35%）</li>
<li>内存吞吐 34.62%（V1 为 27.35%）</li>
<li>实际占用率 39.51%（V1 为 30.13%）</li>
<li>即使提升明显，整体利用率仍较低。</li>
</ul></li>
<li>完整数据导致的内存层级变化：
<ul>
<li>L1 命中率依旧很低（8.96%，与 V1 的 8.85% 相当）</li>
<li>L2 命中率降至 39.95%（V1 为
53.48%），提示新增内核缓存局部性更差</li>
<li>可能与 kernel_0009（记忆高效注意力）和 kernel_0016（CUTLASS
GEMM）的访问模式有关</li>
</ul></li>
<li>内核融合机会：大量小型点算子（乘、加、激活等）共占 21.7%
时间，具备显著融合潜力，可降低访存与启动开销。</li>
<li>混合精度开销：频繁的 BF16 ↔︎ FP32 转换（拷贝/类型转换约
6%）显示数据类型管理可进一步优化。</li>
<li>注意力内核洞见（V2 新发现）：
<ul>
<li>kernel_0009（记忆高效注意力）单次 160.74
μs，归为“均衡”，同时对计算与内存利用率都不高；</li>
<li>相比 FlashAttention（10.34 μs）慢 15.5 倍，提示替换或引入 FA
变体的潜力。</li>
</ul></li>
<li>计算受限内核增多：由 V1 的 3 个增至 4 个（20%），其中
kernel_0016（CUTLASS GEMM）新增；该类内核平均 137.44
μs，远高于内存受限（9.67 μs）与均衡（39.75 μs）。</li>
</ol>
<h3 id="识别出的性能瓶颈">识别出的性能瓶颈</h3>
<ol type="1">
<li>解码阶段 GEMV 效率不足：batch=1 的 GEMV
难以有效利用张量核，也难以跑满内存带宽。</li>
<li>并行度不足：广泛的低占用率显示工作并行度不足以饱和 RTX 5090
的巨大并行资源（21,760 CUDA cores、680 Tensor Cores）。</li>
<li>内存层级利用不足：L1/L2 命中率不高，说明
<ul>
<li>工作集不适配 L1 容量；</li>
<li>时间局部性利用不足；</li>
<li>预取/访问模式可进一步优化。</li>
</ul></li>
<li>内核启动开销：大量短小内核（&lt;5
μs）导致启动开销相对计算时间占比偏高。</li>
</ol>
<h3 id="优化建议软件侧">优化建议（软件侧）</h3>
<ol type="1">
<li>持续批处理（Continuous
Batching）：通过持续批处理或投机解码扩大解码期的 batch，将 GEMV
转化为更高效的 GEMM。</li>
<li>内核融合：
<ul>
<li>融合点算子（乘、加、激活）</li>
<li>融合 LayerNorm + Linear + Activation 常见序列</li>
<li>基于 torch.compile 或自定义 CUDA 内核优化关键路径</li>
</ul></li>
<li>KV-Cache 优化：
<ul>
<li>Paged attention 优化 KV-Cache 访存</li>
<li>采用 FlashDecoding 等面向解码期的优化技术</li>
<li>KV-Cache 量化（INT8/INT4）缓解内存带宽压力</li>
</ul></li>
<li>内存布局优化：
<ul>
<li>尽量减少 BF16 ↔︎ FP32 转换，扩大 BF16 的覆盖面</li>
<li>尽量原地（in-place）操作，降低拷贝</li>
<li>优化张量布局，提升连续访问</li>
</ul></li>
<li>提升并行度：
<ul>
<li>以合批方式提升吞吐（多请求合并）</li>
<li>预填充（prefill）/解码（decode）解耦并在不同实例运行</li>
<li>多查询/分组查询注意力（MQA/GQA）降低解码带宽</li>
</ul></li>
</ol>
<h3 id="面向推理的-npu-设计建议">面向推理的 NPU 设计建议</h3>
<p>基于本次剖析，针对类似 DeepSeek-OCR
的视觉-语言模型，提出如下面向推理优化的 NPU 架构建议：</p>
<h4 id="tensor-core-cuda-core-配比">1. Tensor Core / CUDA Core 配比</h4>
<p>发现：仅 16.7% 的内核为计算受限，且 SM 吞吐仅约 34%。</p>
<p>建议： - 相比训练型 GPU 适度降低张量核密度（例如晶体管面积占比
30-40%，而非 H100 的 50%+） - 增加 CUDA cores
以更好承载点算子与内存受限工作 - 为 GELU、SiLU、LayerNorm
等常用激活/归一化提供专用单元（替代多个分散小内核） - 提供 INT4/INT8
张量核以支持极致量化推理</p>
<h4 id="内存带宽需求">2. 内存带宽需求</h4>
<p>发现：平均内存吞吐 27.35%，内存受限内核平均 41.56%，仍未饱和。</p>
<p>建议： - 相比训练型 GPU
可适当降低带宽配置而不显著影响此类推理工作负载 - 高端推理 NPU 的目标带宽
2-3 TB/s（H100 为 3.35 TB/s） - 将节省下的面积/功耗投入到更大的片上缓存
- 解码期更重视 HBM 延迟而非纯带宽</p>
<h4 id="l1-l2-缓存比例与容量">3. L1 / L2 缓存比例与容量</h4>
<p>发现：L1 命中率 8.85%，L2 命中率 53.48%，表明工作集超出
L1，但可部分驻留在 L2。</p>
<p>建议： - 大幅提升 L2（目标 128-256 MB；A100 为 50 MB） -
缩小或可配置化 L1（与共享内存可互换） - 在 L1 与 L2 之间增设 victim
cache 捕获逐出数据 - 为 KV-Cache 设立高带宽片上 SRAM（32-64 MB）以降低
DRAM 访问 - 针对 Transformer 访问模式（顺序访问
KV-Cache）优化替换策略</p>
<h4 id="专用解码单元">4. 专用解码单元</h4>
<p>发现：49.2% 时间消耗在 GEMV 上，利用率不高。</p>
<p>建议： - 专用 GEMV 加速（batch=1、低时延） -
面向解码配置的脉动阵列：更窄（如 128 列而非
256+）、更深，以流水线并行多个小 GEMV -
权值驻留（weight-stationary）数据流：重复解码步骤下将权值留在片上 -
小规模全连接层尝试片上 SRAM 内计算，进一步消除 DRAM 访问</p>
<h4 id="硬件级算子融合">5. 硬件级算子融合</h4>
<p>发现：21.7% 的时间用于可融合的小型点算子。</p>
<p>建议： - 可编程融合引擎，将多步运算合并，避免往返内存 -
支持常见融合模式：<code>linear → activation</code>、<code>add → layernorm</code>、<code>multiply → add → activation</code>
- 提供宏指令（Macro-op ISA）描述融合序列 -
支持动态控制流，提升内核内分支效率</p>
<h4 id="精度与数据类型支持">6. 精度与数据类型支持</h4>
<p>发现：BF16 为主，但转换带来 ~6% 开销；混合精度普遍存在。</p>
<p>建议： - 全链路原生 BF16 - 硬件加速
FP8（E4M3/E5M2）权值与激活，最小化开销 - 张量核支持 INT4/INT8 极致量化 -
支持每通道动态范围，便于精细量化 -
降低不同格式间的转换开销（理想为单周期）</p>
<h4 id="并行度与占用率">7. 并行度与占用率</h4>
<p>发现：占用率 ~30%，说明并行度不足。</p>
<p>建议： - 相比训练型 GPU 适当减少 SM 数量（如 60-80 vs. H100 的 132）
- 单 SM 内更深的流水以挖掘指令级并行 - 更大的
warp/线程组，摊薄调度与控制开销 -
更强的延迟隐藏机制，针对解码期访问模式优化 -
硬件支持细粒度合批，聚合多条单 token 操作</p>
<h4 id="互连与多芯扩展">8. 互连与多芯扩展</h4>
<p>建议： - 降低芯片间带宽需求（300-500 GB/s，相对 NVLink 900
GB/s），推理更少全规约通信 - 优化管线并行与张量并行，弱化数据并行依赖 -
支持非对称拓扑，将预填充与解码实例分离 - 为预填充-解码解耦架构提供快速
KV-Cache 交换</p>
<h4 id="能效">9. 能效</h4>
<p>建议： - 相比训练 GPU 追求 3-5 倍 TOPS/W
的能效，通过降低张量核密度与带宽实现 -
在内存受限阶段对闲置张量核进行积极时钟门控 - 将 DVFS
策略围绕推理时延目标（如 P99）而非吞吐进行优化 -
提供解码/预填充阶段的差异化低功耗模式</p>
<hr />
<p>剖析产物：所有原始数据、指标、直方图、屋脊线图与分析脚本均位于
<code>reports/20251107-dsocr/ncu-v2/analysis/</code>。</p>
<hr />
<h2 id="附录完整内核函数名">附录：完整内核函数名</h2>
<p>本附录给出完整（未截断）的内核函数名，便于溯源与调试。对应“按总时间排序的内核”章节中的条目。</p>
<h3 id="内核名称映射表">内核名称映射表</h3>
<p>下表将易读的内核名映射到分析工具输出中的完整（经名称重整）函数签名。</p>
<table>
<colgroup>
<col style="width: 13%" />
<col style="width: 21%" />
<col style="width: 26%" />
<col style="width: 8%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr>
<th>排名</th>
<th>时间占比</th>
<th>易读内核名</th>
<th>库</th>
<th>完整函数签名</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>34.4%</td>
<td>GEMV-1 (BF16, template=7)</td>
<td>cuBLAS</td>
<td><code>std::enable_if&lt;!T7, void&gt;::type internal::gemvx::kernel&lt;int, int, __nv_bfloat16, __nv_bfloat16, __nv_bfloat16, float, (bool)0, (bool)1, (bool)1, (bool)0, (int)7, (bool)0, cublasGemvParamsEx&lt;int, cublasGemvTensorStridedBatched&lt;const __nv_bfloat16&gt;, cublasGemvTensorStridedBatched&lt;const __nv_bfloat16&gt;, cublasGemvTensorStridedBatched&lt;__nv_bfloat16&gt;, float&gt;&gt;(T13)</code></td>
</tr>
<tr>
<td>2</td>
<td>14.8%</td>
<td>GEMV-2 (BF16, template=6)</td>
<td>cuBLAS</td>
<td><code>std::enable_if&lt;!T7, void&gt;::type internal::gemvx::kernel&lt;int, int, __nv_bfloat16, __nv_bfloat16, __nv_bfloat16, float, (bool)0, (bool)1, (bool)1, (bool)0, (int)6, (bool)0, cublasGemvParamsEx&lt;int, cublasGemvTensorStridedBatched&lt;const __nv_bfloat16&gt;, cublasGemvTensorStridedBatched&lt;const __nv_bfloat16&gt;, cublasGemvTensorStridedBatched&lt;__nv_bfloat16&gt;, float&gt;&gt;(T13)</code></td>
</tr>
<tr>
<td>3</td>
<td>6.0%</td>
<td>Direct Copy (float)</td>
<td>PyTorch ATen</td>
<td><code>void at::native::unrolled_elementwise_kernel&lt;at::native::direct_copy_kernel_cuda(at::TensorIteratorBase &amp;)::[lambda() (instance 3)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)], std::array&lt;char *, (unsigned long)2&gt;, (int)4, TrivialOffsetCalculator&lt;(int)1, unsigned int&gt;, TrivialOffsetCalculator&lt;(int)1, unsigned int&gt;, at::native::memory::LoadWithCast&lt;(int)1&gt;, at::native::memory::StoreWithCast&lt;(int)1&gt;&gt;(int, T1, T2, T4, T5, T6, T7)</code></td>
</tr>
<tr>
<td>4</td>
<td>4.2%</td>
<td>Elementwise Multiply (BF16, vec)</td>
<td>PyTorch ATen</td>
<td><code>void at::native::vectorized_elementwise_kernel&lt;(int)4, at::native::BinaryFunctor&lt;c10::BFloat16, c10::BFloat16, c10::BFloat16, at::native::binary_internal::MulFunctor&lt;float&gt;&gt;, std::array&lt;char *, (unsigned long)3&gt;&gt;(int, T2, T3)</code></td>
</tr>
<tr>
<td>5</td>
<td>3.9%</td>
<td>SiLU Activation (BF16, vec)</td>
<td>PyTorch ATen</td>
<td><code>void at::native::vectorized_elementwise_kernel&lt;(int)4, at::native::&lt;unnamed&gt;::silu_kernel(at::TensorIteratorBase &amp;)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 6)]::operator ()() const::[lambda(c10::BFloat16) (instance 1)], std::array&lt;char *, (unsigned long)2&gt;&gt;(int, T2, T3)</code></td>
</tr>
<tr>
<td>6</td>
<td>2.9%</td>
<td>Elementwise Multiply (BF16)</td>
<td>PyTorch ATen</td>
<td><code>void at::native::elementwise_kernel&lt;(int)128, (int)4, void at::native::gpu_kernel_impl_nocast&lt;at::native::BinaryFunctor&lt;c10::BFloat16, c10::BFloat16, c10::BFloat16, at::native::binary_internal::MulFunctor&lt;float&gt;&gt;&gt;(at::TensorIteratorBase &amp;, const T1 &amp;)::[lambda(int) (instance 1)]&gt;(int, T3)</code></td>
</tr>
<tr>
<td>7</td>
<td>2.8%</td>
<td>Cat Batched Copy (vec, 128-tile)</td>
<td>PyTorch ATen</td>
<td><code>void at::native::&lt;unnamed&gt;::CatArrayBatchedCopy_vectorized&lt;at::native::&lt;unnamed&gt;::OpaqueType&lt;(unsigned int)2&gt;, unsigned int, (int)3, (int)128, (int)1, (int)16, (int)8&gt;(char *, at::native::&lt;unnamed&gt;::CatArrInputTensorMetadata&lt;T1, T2, T4, T5&gt;, at::native::&lt;unnamed&gt;::TensorSizeStride&lt;T2, (unsigned int)4&gt;, int, T2)</code></td>
</tr>
<tr>
<td>8</td>
<td>2.7%</td>
<td>Copy/Cast (BF16, vec)</td>
<td>PyTorch ATen</td>
<td><code>void at::native::vectorized_elementwise_kernel&lt;(int)4, at::native::bfloat16_copy_kernel_cuda(at::TensorIteratorBase &amp;)::[lambda(float) (instance 1)], std::array&lt;char *, (unsigned long)2&gt;&gt;(int, T2, T3)</code></td>
</tr>
<tr>
<td>9</td>
<td>2.4%</td>
<td>Flash Forward Split-KV (BF16)</td>
<td>FlashAttention</td>
<td><code>void flash::flash_fwd_splitkv_kernel&lt;Flash_fwd_kernel_traits&lt;(int)128, (int)64, (int)128, (int)4, (bool)0, (bool)0, cutlass::bfloat16_t, Flash_kernel_traits&lt;(int)128, (int)64, (int)128, (int)4, cutlass::bfloat16_t&gt;&gt;, (bool)0, (bool)0, (bool)0, (bool)0, (bool)1, (bool)0, (bool)1, (bool)0&gt;(flash::Flash_fwd_params)</code></td>
</tr>
<tr>
<td>10</td>
<td>2.0%</td>
<td>Elementwise Add (BF16, vec)</td>
<td>PyTorch ATen</td>
<td><code>void at::native::vectorized_elementwise_kernel&lt;(int)4, at::native::CUDAFunctor_add&lt;c10::BFloat16&gt;, std::array&lt;char *, (unsigned long)3&gt;&gt;(int, T2, T3)</code></td>
</tr>
<tr>
<td>11</td>
<td>1.8%</td>
<td>Cat Batched Copy (vec, 64-tile)</td>
<td>PyTorch ATen</td>
<td><code>void at::native::&lt;unnamed&gt;::CatArrayBatchedCopy&lt;at::native::&lt;unnamed&gt;::OpaqueType&lt;(unsigned int)2&gt;, unsigned int, (int)4, (int)64, (int)64&gt;(T1 *, at::native::&lt;unnamed&gt;::CatArrInputTensorMetadata&lt;T1, T2, T4, T5&gt;, at::native::&lt;unnamed&gt;::TensorSizeStride&lt;T2, (unsigned int)4&gt;, int, T2)</code></td>
</tr>
<tr>
<td>12</td>
<td>1.8%</td>
<td>Elementwise Multiply (float)</td>
<td>PyTorch ATen</td>
<td><code>void at::native::elementwise_kernel&lt;(int)128, (int)2, void at::native::gpu_kernel_impl_nocast&lt;at::native::BinaryFunctor&lt;float, float, float, at::native::binary_internal::MulFunctor&lt;float&gt;&gt;&gt;(at::TensorIteratorBase &amp;, const T1 &amp;)::[lambda(int) (instance 1)]&gt;(int, T3)</code></td>
</tr>
<tr>
<td>13</td>
<td>1.6%</td>
<td>Mean Reduction (float)</td>
<td>PyTorch ATen</td>
<td><code>void at::native::reduce_kernel&lt;(int)512, (int)1, at::native::ReduceOp&lt;float, at::native::MeanOps&lt;float, float, float, float&gt;, unsigned int, float, (int)4, (int)4&gt;&gt;(T3)</code></td>
</tr>
<tr>
<td>14</td>
<td>1.5%</td>
<td>Elementwise Neg (BF16)</td>
<td>PyTorch ATen</td>
<td><code>void at::native::elementwise_kernel&lt;(int)128, (int)4, void at::native::gpu_kernel_impl_nocast&lt;at::native::neg_kernel_cuda(at::TensorIteratorBase &amp;)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 9)]::operator ()() const::[lambda(c10::BFloat16) (instance 1)]&gt;(at::TensorIteratorBase &amp;, const T1 &amp;)::[lambda(int) (instance 1)]&gt;(int, T3)</code></td>
</tr>
<tr>
<td>15</td>
<td>1.4%</td>
<td>Flash Split-KV Combine</td>
<td>FlashAttention</td>
<td><code>void flash::flash_fwd_splitkv_combine_kernel&lt;Flash_fwd_kernel_traits&lt;(int)128, (int)64, (int)128, (int)4, (bool)0, (bool)0, cutlass::bfloat16_t, Flash_kernel_traits&lt;(int)128, (int)64, (int)128, (int)4, cutlass::bfloat16_t&gt;&gt;, (int)4, (int)3, (bool)1&gt;(flash::Flash_fwd_params)</code></td>
</tr>
</tbody>
</table>
<h3 id="函数名解读说明">函数名解读说明</h3>
<p>模板参数： - 尖括号 <code>&lt; &gt;</code> 中的值为编译期配置 -
数据类型：<code>__nv_bfloat16</code>、<code>float</code>、<code>c10::BFloat16</code>、<code>cutlass::bfloat16_t</code>
- 分块/铺片尺寸：如 <code>(int)128, (int)64, (int)128, (int)4</code>
对应 block/warp/thread 等配置 -
布尔开关：<code>(bool)0</code>（false）、<code>(bool)1</code>（true）控制内核变体</p>
<p>名称重整（Mangled）： -
<code>T1</code>、<code>T2</code>、<code>T3</code> 等为 C++
模板参数占位符 - cuBLAS 中的 <code>T13</code> 表示参数包展开</p>
<p>Lambda 表达式： - <code>[lambda() (instance N)]</code> 为 PyTorch
函数式风格生成的匿名函数 - 通过实例编号区分相似签名的多个 lambda</p>
<p>命名空间前缀： - <code>at::native::</code> - PyTorch ATen（A Tensor
Library）原生 CUDA 内核 - <code>internal::gemvx::</code> - cuBLAS 内部
GEMV 实现 - <code>flash::</code> - FlashAttention 内核</p>
<p>实践用途： 1. 在 Nsight Compute <code>.ncu-rep</code> 或 CSV
中进行精确字符串匹配 2. 复制完整签名以过滤分析输出 3. 追踪至
PyTorch/cuBLAS/CUTLASS 的具体模板实例 4. 与
<code>ncu-v2/analysis/roofline_per_kernel/</code>
中的每内核屋脊线图交叉引用 5.
解析函数签名以抽取配置参数用于自动化分析</p>
<p>相似内核的关键差异： - GEMV-1 vs GEMV-2：模板参数 <code>(int)7</code>
vs <code>(int)6</code>，影响内部铺片策略 - Cat Batched Copy
变体：<code>(int)3, (int)128</code> vs
<code>(int)4, (int)64</code>，指定拷贝维度与分块大小 - 向量化 vs
非向量化点算子：<code>vectorized_elementwise_kernel</code>
使用向量装载/存储（<code>(int)4</code> 表示 4 元向量）</p>
</body>
</html>
