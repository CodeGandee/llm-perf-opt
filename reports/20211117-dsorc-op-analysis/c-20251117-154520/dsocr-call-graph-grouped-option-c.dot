digraph dsocr_grouped_option_c {
  rankdir=LR;
  nodesep=0.3;
  ranksep=0.7;
  "Block @ sam_model.blocks.[0-11] for 12" [shape=box, style=filled, fillcolor=lightgray];
  "ImageEncoderViT @ sam_model" -> "Block @ sam_model.blocks.[0-11]" [label="parfor 1504"];
  "Block @ sam_model.blocks.0" -> "LayerNorm @ sam_model.blocks.[0-0].norm1" [label="parfor 2"];
  "Block @ sam_model.blocks.0" -> "Attention @ sam_model.blocks.[0-0].attn" [label="parfor 92"];
  "Attention @ sam_model.blocks.0.attn" -> "Linear @ sam_model.blocks.[0-0].attn.qkv" [label="parfor 2"];
  "Attention @ sam_model.blocks.0.attn" -> "Linear @ sam_model.blocks.[0-0].attn.proj" [label="parfor 2"];
  "Block @ sam_model.blocks.0" -> "LayerNorm @ sam_model.blocks.[0-0].norm2" [label="parfor 2"];
  "Block @ sam_model.blocks.0" -> "MLPBlock @ sam_model.blocks.[0-0].mlp" [label="parfor 6"];
  "MLPBlock @ sam_model.blocks.0.mlp" -> "Linear @ sam_model.blocks.[0-0].mlp.lin1" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.0.mlp" -> "GELU @ sam_model.blocks.[0-0].mlp.act" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.0.mlp" -> "Linear @ sam_model.blocks.[0-0].mlp.lin2" [label="parfor 2"];
  "Block @ sam_model.blocks.1" -> "LayerNorm @ sam_model.blocks.[1-1].norm1" [label="parfor 2"];
  "Block @ sam_model.blocks.1" -> "Attention @ sam_model.blocks.[1-1].attn" [label="parfor 92"];
  "Attention @ sam_model.blocks.1.attn" -> "Linear @ sam_model.blocks.[1-1].attn.qkv" [label="parfor 2"];
  "Attention @ sam_model.blocks.1.attn" -> "Linear @ sam_model.blocks.[1-1].attn.proj" [label="parfor 2"];
  "Block @ sam_model.blocks.1" -> "LayerNorm @ sam_model.blocks.[1-1].norm2" [label="parfor 2"];
  "Block @ sam_model.blocks.1" -> "MLPBlock @ sam_model.blocks.[1-1].mlp" [label="parfor 6"];
  "MLPBlock @ sam_model.blocks.1.mlp" -> "Linear @ sam_model.blocks.[1-1].mlp.lin1" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.1.mlp" -> "GELU @ sam_model.blocks.[1-1].mlp.act" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.1.mlp" -> "Linear @ sam_model.blocks.[1-1].mlp.lin2" [label="parfor 2"];
  "Block @ sam_model.blocks.2" -> "LayerNorm @ sam_model.blocks.[2-2].norm1" [label="parfor 2"];
  "Block @ sam_model.blocks.2" -> "Attention @ sam_model.blocks.[2-2].attn" [label="parfor 106"];
  "Attention @ sam_model.blocks.2.attn" -> "Linear @ sam_model.blocks.[2-2].attn.qkv" [label="parfor 2"];
  "Attention @ sam_model.blocks.2.attn" -> "Linear @ sam_model.blocks.[2-2].attn.proj" [label="parfor 2"];
  "Block @ sam_model.blocks.2" -> "LayerNorm @ sam_model.blocks.[2-2].norm2" [label="parfor 2"];
  "Block @ sam_model.blocks.2" -> "MLPBlock @ sam_model.blocks.[2-2].mlp" [label="parfor 6"];
  "MLPBlock @ sam_model.blocks.2.mlp" -> "Linear @ sam_model.blocks.[2-2].mlp.lin1" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.2.mlp" -> "GELU @ sam_model.blocks.[2-2].mlp.act" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.2.mlp" -> "Linear @ sam_model.blocks.[2-2].mlp.lin2" [label="parfor 2"];
  "Block @ sam_model.blocks.3" -> "LayerNorm @ sam_model.blocks.[3-3].norm1" [label="parfor 2"];
  "Block @ sam_model.blocks.3" -> "Attention @ sam_model.blocks.[3-3].attn" [label="parfor 92"];
  "Attention @ sam_model.blocks.3.attn" -> "Linear @ sam_model.blocks.[3-3].attn.qkv" [label="parfor 2"];
  "Attention @ sam_model.blocks.3.attn" -> "Linear @ sam_model.blocks.[3-3].attn.proj" [label="parfor 2"];
  "Block @ sam_model.blocks.3" -> "LayerNorm @ sam_model.blocks.[3-3].norm2" [label="parfor 2"];
  "Block @ sam_model.blocks.3" -> "MLPBlock @ sam_model.blocks.[3-3].mlp" [label="parfor 6"];
  "MLPBlock @ sam_model.blocks.3.mlp" -> "Linear @ sam_model.blocks.[3-3].mlp.lin1" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.3.mlp" -> "GELU @ sam_model.blocks.[3-3].mlp.act" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.3.mlp" -> "Linear @ sam_model.blocks.[3-3].mlp.lin2" [label="parfor 2"];
  "Block @ sam_model.blocks.4" -> "LayerNorm @ sam_model.blocks.[4-4].norm1" [label="parfor 2"];
  "Block @ sam_model.blocks.4" -> "Attention @ sam_model.blocks.[4-4].attn" [label="parfor 92"];
  "Attention @ sam_model.blocks.4.attn" -> "Linear @ sam_model.blocks.[4-4].attn.qkv" [label="parfor 2"];
  "Attention @ sam_model.blocks.4.attn" -> "Linear @ sam_model.blocks.[4-4].attn.proj" [label="parfor 2"];
  "Block @ sam_model.blocks.4" -> "LayerNorm @ sam_model.blocks.[4-4].norm2" [label="parfor 2"];
  "Block @ sam_model.blocks.4" -> "MLPBlock @ sam_model.blocks.[4-4].mlp" [label="parfor 6"];
  "MLPBlock @ sam_model.blocks.4.mlp" -> "Linear @ sam_model.blocks.[4-4].mlp.lin1" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.4.mlp" -> "GELU @ sam_model.blocks.[4-4].mlp.act" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.4.mlp" -> "Linear @ sam_model.blocks.[4-4].mlp.lin2" [label="parfor 2"];
  "Block @ sam_model.blocks.5" -> "LayerNorm @ sam_model.blocks.[5-5].norm1" [label="parfor 2"];
  "Block @ sam_model.blocks.5" -> "Attention @ sam_model.blocks.[5-5].attn" [label="parfor 106"];
  "Attention @ sam_model.blocks.5.attn" -> "Linear @ sam_model.blocks.[5-5].attn.qkv" [label="parfor 2"];
  "Attention @ sam_model.blocks.5.attn" -> "Linear @ sam_model.blocks.[5-5].attn.proj" [label="parfor 2"];
  "Block @ sam_model.blocks.5" -> "LayerNorm @ sam_model.blocks.[5-5].norm2" [label="parfor 2"];
  "Block @ sam_model.blocks.5" -> "MLPBlock @ sam_model.blocks.[5-5].mlp" [label="parfor 6"];
  "MLPBlock @ sam_model.blocks.5.mlp" -> "Linear @ sam_model.blocks.[5-5].mlp.lin1" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.5.mlp" -> "GELU @ sam_model.blocks.[5-5].mlp.act" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.5.mlp" -> "Linear @ sam_model.blocks.[5-5].mlp.lin2" [label="parfor 2"];
  "Block @ sam_model.blocks.6" -> "LayerNorm @ sam_model.blocks.[6-6].norm1" [label="parfor 2"];
  "Block @ sam_model.blocks.6" -> "Attention @ sam_model.blocks.[6-6].attn" [label="parfor 92"];
  "Attention @ sam_model.blocks.6.attn" -> "Linear @ sam_model.blocks.[6-6].attn.qkv" [label="parfor 2"];
  "Attention @ sam_model.blocks.6.attn" -> "Linear @ sam_model.blocks.[6-6].attn.proj" [label="parfor 2"];
  "Block @ sam_model.blocks.6" -> "LayerNorm @ sam_model.blocks.[6-6].norm2" [label="parfor 2"];
  "Block @ sam_model.blocks.6" -> "MLPBlock @ sam_model.blocks.[6-6].mlp" [label="parfor 6"];
  "MLPBlock @ sam_model.blocks.6.mlp" -> "Linear @ sam_model.blocks.[6-6].mlp.lin1" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.6.mlp" -> "GELU @ sam_model.blocks.[6-6].mlp.act" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.6.mlp" -> "Linear @ sam_model.blocks.[6-6].mlp.lin2" [label="parfor 2"];
  "Block @ sam_model.blocks.7" -> "LayerNorm @ sam_model.blocks.[7-7].norm1" [label="parfor 2"];
  "Block @ sam_model.blocks.7" -> "Attention @ sam_model.blocks.[7-7].attn" [label="parfor 92"];
  "Attention @ sam_model.blocks.7.attn" -> "Linear @ sam_model.blocks.[7-7].attn.qkv" [label="parfor 2"];
  "Attention @ sam_model.blocks.7.attn" -> "Linear @ sam_model.blocks.[7-7].attn.proj" [label="parfor 2"];
  "Block @ sam_model.blocks.7" -> "LayerNorm @ sam_model.blocks.[7-7].norm2" [label="parfor 2"];
  "Block @ sam_model.blocks.7" -> "MLPBlock @ sam_model.blocks.[7-7].mlp" [label="parfor 6"];
  "MLPBlock @ sam_model.blocks.7.mlp" -> "Linear @ sam_model.blocks.[7-7].mlp.lin1" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.7.mlp" -> "GELU @ sam_model.blocks.[7-7].mlp.act" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.7.mlp" -> "Linear @ sam_model.blocks.[7-7].mlp.lin2" [label="parfor 2"];
  "Block @ sam_model.blocks.8" -> "LayerNorm @ sam_model.blocks.[8-8].norm1" [label="parfor 2"];
  "Block @ sam_model.blocks.8" -> "Attention @ sam_model.blocks.[8-8].attn" [label="parfor 106"];
  "Attention @ sam_model.blocks.8.attn" -> "Linear @ sam_model.blocks.[8-8].attn.qkv" [label="parfor 2"];
  "Attention @ sam_model.blocks.8.attn" -> "Linear @ sam_model.blocks.[8-8].attn.proj" [label="parfor 2"];
  "Block @ sam_model.blocks.8" -> "LayerNorm @ sam_model.blocks.[8-8].norm2" [label="parfor 2"];
  "Block @ sam_model.blocks.8" -> "MLPBlock @ sam_model.blocks.[8-8].mlp" [label="parfor 6"];
  "MLPBlock @ sam_model.blocks.8.mlp" -> "Linear @ sam_model.blocks.[8-8].mlp.lin1" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.8.mlp" -> "GELU @ sam_model.blocks.[8-8].mlp.act" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.8.mlp" -> "Linear @ sam_model.blocks.[8-8].mlp.lin2" [label="parfor 2"];
  "Block @ sam_model.blocks.9" -> "LayerNorm @ sam_model.blocks.[9-9].norm1" [label="parfor 2"];
  "Block @ sam_model.blocks.9" -> "Attention @ sam_model.blocks.[9-9].attn" [label="parfor 92"];
  "Attention @ sam_model.blocks.9.attn" -> "Linear @ sam_model.blocks.[9-9].attn.qkv" [label="parfor 2"];
  "Attention @ sam_model.blocks.9.attn" -> "Linear @ sam_model.blocks.[9-9].attn.proj" [label="parfor 2"];
  "Block @ sam_model.blocks.9" -> "LayerNorm @ sam_model.blocks.[9-9].norm2" [label="parfor 2"];
  "Block @ sam_model.blocks.9" -> "MLPBlock @ sam_model.blocks.[9-9].mlp" [label="parfor 6"];
  "MLPBlock @ sam_model.blocks.9.mlp" -> "Linear @ sam_model.blocks.[9-9].mlp.lin1" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.9.mlp" -> "GELU @ sam_model.blocks.[9-9].mlp.act" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.9.mlp" -> "Linear @ sam_model.blocks.[9-9].mlp.lin2" [label="parfor 2"];
  "Block @ sam_model.blocks.10" -> "LayerNorm @ sam_model.blocks.[10-10].norm1" [label="parfor 2"];
  "Block @ sam_model.blocks.10" -> "Attention @ sam_model.blocks.[10-10].attn" [label="parfor 92"];
  "Attention @ sam_model.blocks.10.attn" -> "Linear @ sam_model.blocks.[10-10].attn.qkv" [label="parfor 2"];
  "Attention @ sam_model.blocks.10.attn" -> "Linear @ sam_model.blocks.[10-10].attn.proj" [label="parfor 2"];
  "Block @ sam_model.blocks.10" -> "LayerNorm @ sam_model.blocks.[10-10].norm2" [label="parfor 2"];
  "Block @ sam_model.blocks.10" -> "MLPBlock @ sam_model.blocks.[10-10].mlp" [label="parfor 6"];
  "MLPBlock @ sam_model.blocks.10.mlp" -> "Linear @ sam_model.blocks.[10-10].mlp.lin1" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.10.mlp" -> "GELU @ sam_model.blocks.[10-10].mlp.act" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.10.mlp" -> "Linear @ sam_model.blocks.[10-10].mlp.lin2" [label="parfor 2"];
  "Block @ sam_model.blocks.11" -> "LayerNorm @ sam_model.blocks.[11-11].norm1" [label="parfor 2"];
  "Block @ sam_model.blocks.11" -> "Attention @ sam_model.blocks.[11-11].attn" [label="parfor 106"];
  "Attention @ sam_model.blocks.11.attn" -> "Linear @ sam_model.blocks.[11-11].attn.qkv" [label="parfor 2"];
  "Attention @ sam_model.blocks.11.attn" -> "Linear @ sam_model.blocks.[11-11].attn.proj" [label="parfor 2"];
  "Block @ sam_model.blocks.11" -> "LayerNorm @ sam_model.blocks.[11-11].norm2" [label="parfor 2"];
  "Block @ sam_model.blocks.11" -> "MLPBlock @ sam_model.blocks.[11-11].mlp" [label="parfor 6"];
  "MLPBlock @ sam_model.blocks.11.mlp" -> "Linear @ sam_model.blocks.[11-11].mlp.lin1" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.11.mlp" -> "GELU @ sam_model.blocks.[11-11].mlp.act" [label="parfor 2"];
  "MLPBlock @ sam_model.blocks.11.mlp" -> "Linear @ sam_model.blocks.[11-11].mlp.lin2" [label="parfor 2"];
  "Sequential @ sam_model.neck" -> "Conv2d @ sam_model.neck.[0,2]" [label="parfor 4"];
  "Sequential @ sam_model.neck" -> "LayerNorm2d @ sam_model.neck.[1,3]" [label="parfor 48"];
  "NoTPTransformer @ vision_model.transformer" -> "NoTPTransformerBlock @ vision_model.transformer.layers.[0-23]" [label="parfor 1152"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.0" -> "LayerNorm @ vision_model.transformer.layers.[0-0].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.0" -> "NoTPAttention @ vision_model.transformer.layers.[0-0].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.0.self_attn" -> "Linear @ vision_model.transformer.layers.[0-0].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.0.self_attn" -> "Linear @ vision_model.transformer.layers.[0-0].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.0" -> "LayerNorm @ vision_model.transformer.layers.[0-0].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.0" -> "NoTPFeedForward @ vision_model.transformer.layers.[0-0].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.0.mlp" -> "Linear @ vision_model.transformer.layers.[0-0].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.0.mlp" -> "Linear @ vision_model.transformer.layers.[0-0].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.1" -> "LayerNorm @ vision_model.transformer.layers.[1-1].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.1" -> "NoTPAttention @ vision_model.transformer.layers.[1-1].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.1.self_attn" -> "Linear @ vision_model.transformer.layers.[1-1].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.1.self_attn" -> "Linear @ vision_model.transformer.layers.[1-1].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.1" -> "LayerNorm @ vision_model.transformer.layers.[1-1].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.1" -> "NoTPFeedForward @ vision_model.transformer.layers.[1-1].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.1.mlp" -> "Linear @ vision_model.transformer.layers.[1-1].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.1.mlp" -> "Linear @ vision_model.transformer.layers.[1-1].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.2" -> "LayerNorm @ vision_model.transformer.layers.[2-2].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.2" -> "NoTPAttention @ vision_model.transformer.layers.[2-2].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.2.self_attn" -> "Linear @ vision_model.transformer.layers.[2-2].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.2.self_attn" -> "Linear @ vision_model.transformer.layers.[2-2].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.2" -> "LayerNorm @ vision_model.transformer.layers.[2-2].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.2" -> "NoTPFeedForward @ vision_model.transformer.layers.[2-2].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.2.mlp" -> "Linear @ vision_model.transformer.layers.[2-2].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.2.mlp" -> "Linear @ vision_model.transformer.layers.[2-2].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.3" -> "LayerNorm @ vision_model.transformer.layers.[3-3].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.3" -> "NoTPAttention @ vision_model.transformer.layers.[3-3].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.3.self_attn" -> "Linear @ vision_model.transformer.layers.[3-3].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.3.self_attn" -> "Linear @ vision_model.transformer.layers.[3-3].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.3" -> "LayerNorm @ vision_model.transformer.layers.[3-3].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.3" -> "NoTPFeedForward @ vision_model.transformer.layers.[3-3].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.3.mlp" -> "Linear @ vision_model.transformer.layers.[3-3].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.3.mlp" -> "Linear @ vision_model.transformer.layers.[3-3].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.4" -> "LayerNorm @ vision_model.transformer.layers.[4-4].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.4" -> "NoTPAttention @ vision_model.transformer.layers.[4-4].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.4.self_attn" -> "Linear @ vision_model.transformer.layers.[4-4].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.4.self_attn" -> "Linear @ vision_model.transformer.layers.[4-4].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.4" -> "LayerNorm @ vision_model.transformer.layers.[4-4].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.4" -> "NoTPFeedForward @ vision_model.transformer.layers.[4-4].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.4.mlp" -> "Linear @ vision_model.transformer.layers.[4-4].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.4.mlp" -> "Linear @ vision_model.transformer.layers.[4-4].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.5" -> "LayerNorm @ vision_model.transformer.layers.[5-5].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.5" -> "NoTPAttention @ vision_model.transformer.layers.[5-5].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.5.self_attn" -> "Linear @ vision_model.transformer.layers.[5-5].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.5.self_attn" -> "Linear @ vision_model.transformer.layers.[5-5].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.5" -> "LayerNorm @ vision_model.transformer.layers.[5-5].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.5" -> "NoTPFeedForward @ vision_model.transformer.layers.[5-5].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.5.mlp" -> "Linear @ vision_model.transformer.layers.[5-5].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.5.mlp" -> "Linear @ vision_model.transformer.layers.[5-5].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.6" -> "LayerNorm @ vision_model.transformer.layers.[6-6].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.6" -> "NoTPAttention @ vision_model.transformer.layers.[6-6].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.6.self_attn" -> "Linear @ vision_model.transformer.layers.[6-6].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.6.self_attn" -> "Linear @ vision_model.transformer.layers.[6-6].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.6" -> "LayerNorm @ vision_model.transformer.layers.[6-6].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.6" -> "NoTPFeedForward @ vision_model.transformer.layers.[6-6].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.6.mlp" -> "Linear @ vision_model.transformer.layers.[6-6].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.6.mlp" -> "Linear @ vision_model.transformer.layers.[6-6].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.7" -> "LayerNorm @ vision_model.transformer.layers.[7-7].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.7" -> "NoTPAttention @ vision_model.transformer.layers.[7-7].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.7.self_attn" -> "Linear @ vision_model.transformer.layers.[7-7].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.7.self_attn" -> "Linear @ vision_model.transformer.layers.[7-7].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.7" -> "LayerNorm @ vision_model.transformer.layers.[7-7].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.7" -> "NoTPFeedForward @ vision_model.transformer.layers.[7-7].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.7.mlp" -> "Linear @ vision_model.transformer.layers.[7-7].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.7.mlp" -> "Linear @ vision_model.transformer.layers.[7-7].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.8" -> "LayerNorm @ vision_model.transformer.layers.[8-8].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.8" -> "NoTPAttention @ vision_model.transformer.layers.[8-8].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.8.self_attn" -> "Linear @ vision_model.transformer.layers.[8-8].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.8.self_attn" -> "Linear @ vision_model.transformer.layers.[8-8].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.8" -> "LayerNorm @ vision_model.transformer.layers.[8-8].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.8" -> "NoTPFeedForward @ vision_model.transformer.layers.[8-8].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.8.mlp" -> "Linear @ vision_model.transformer.layers.[8-8].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.8.mlp" -> "Linear @ vision_model.transformer.layers.[8-8].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.9" -> "LayerNorm @ vision_model.transformer.layers.[9-9].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.9" -> "NoTPAttention @ vision_model.transformer.layers.[9-9].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.9.self_attn" -> "Linear @ vision_model.transformer.layers.[9-9].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.9.self_attn" -> "Linear @ vision_model.transformer.layers.[9-9].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.9" -> "LayerNorm @ vision_model.transformer.layers.[9-9].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.9" -> "NoTPFeedForward @ vision_model.transformer.layers.[9-9].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.9.mlp" -> "Linear @ vision_model.transformer.layers.[9-9].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.9.mlp" -> "Linear @ vision_model.transformer.layers.[9-9].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.10" -> "LayerNorm @ vision_model.transformer.layers.[10-10].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.10" -> "NoTPAttention @ vision_model.transformer.layers.[10-10].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.10.self_attn" -> "Linear @ vision_model.transformer.layers.[10-10].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.10.self_attn" -> "Linear @ vision_model.transformer.layers.[10-10].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.10" -> "LayerNorm @ vision_model.transformer.layers.[10-10].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.10" -> "NoTPFeedForward @ vision_model.transformer.layers.[10-10].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.10.mlp" -> "Linear @ vision_model.transformer.layers.[10-10].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.10.mlp" -> "Linear @ vision_model.transformer.layers.[10-10].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.11" -> "LayerNorm @ vision_model.transformer.layers.[11-11].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.11" -> "NoTPAttention @ vision_model.transformer.layers.[11-11].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.11.self_attn" -> "Linear @ vision_model.transformer.layers.[11-11].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.11.self_attn" -> "Linear @ vision_model.transformer.layers.[11-11].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.11" -> "LayerNorm @ vision_model.transformer.layers.[11-11].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.11" -> "NoTPFeedForward @ vision_model.transformer.layers.[11-11].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.11.mlp" -> "Linear @ vision_model.transformer.layers.[11-11].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.11.mlp" -> "Linear @ vision_model.transformer.layers.[11-11].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.12" -> "LayerNorm @ vision_model.transformer.layers.[12-12].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.12" -> "NoTPAttention @ vision_model.transformer.layers.[12-12].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.12.self_attn" -> "Linear @ vision_model.transformer.layers.[12-12].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.12.self_attn" -> "Linear @ vision_model.transformer.layers.[12-12].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.12" -> "LayerNorm @ vision_model.transformer.layers.[12-12].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.12" -> "NoTPFeedForward @ vision_model.transformer.layers.[12-12].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.12.mlp" -> "Linear @ vision_model.transformer.layers.[12-12].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.12.mlp" -> "Linear @ vision_model.transformer.layers.[12-12].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.13" -> "LayerNorm @ vision_model.transformer.layers.[13-13].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.13" -> "NoTPAttention @ vision_model.transformer.layers.[13-13].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.13.self_attn" -> "Linear @ vision_model.transformer.layers.[13-13].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.13.self_attn" -> "Linear @ vision_model.transformer.layers.[13-13].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.13" -> "LayerNorm @ vision_model.transformer.layers.[13-13].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.13" -> "NoTPFeedForward @ vision_model.transformer.layers.[13-13].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.13.mlp" -> "Linear @ vision_model.transformer.layers.[13-13].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.13.mlp" -> "Linear @ vision_model.transformer.layers.[13-13].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.14" -> "LayerNorm @ vision_model.transformer.layers.[14-14].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.14" -> "NoTPAttention @ vision_model.transformer.layers.[14-14].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.14.self_attn" -> "Linear @ vision_model.transformer.layers.[14-14].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.14.self_attn" -> "Linear @ vision_model.transformer.layers.[14-14].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.14" -> "LayerNorm @ vision_model.transformer.layers.[14-14].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.14" -> "NoTPFeedForward @ vision_model.transformer.layers.[14-14].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.14.mlp" -> "Linear @ vision_model.transformer.layers.[14-14].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.14.mlp" -> "Linear @ vision_model.transformer.layers.[14-14].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.15" -> "LayerNorm @ vision_model.transformer.layers.[15-15].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.15" -> "NoTPAttention @ vision_model.transformer.layers.[15-15].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.15.self_attn" -> "Linear @ vision_model.transformer.layers.[15-15].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.15.self_attn" -> "Linear @ vision_model.transformer.layers.[15-15].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.15" -> "LayerNorm @ vision_model.transformer.layers.[15-15].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.15" -> "NoTPFeedForward @ vision_model.transformer.layers.[15-15].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.15.mlp" -> "Linear @ vision_model.transformer.layers.[15-15].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.15.mlp" -> "Linear @ vision_model.transformer.layers.[15-15].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.16" -> "LayerNorm @ vision_model.transformer.layers.[16-16].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.16" -> "NoTPAttention @ vision_model.transformer.layers.[16-16].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.16.self_attn" -> "Linear @ vision_model.transformer.layers.[16-16].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.16.self_attn" -> "Linear @ vision_model.transformer.layers.[16-16].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.16" -> "LayerNorm @ vision_model.transformer.layers.[16-16].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.16" -> "NoTPFeedForward @ vision_model.transformer.layers.[16-16].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.16.mlp" -> "Linear @ vision_model.transformer.layers.[16-16].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.16.mlp" -> "Linear @ vision_model.transformer.layers.[16-16].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.17" -> "LayerNorm @ vision_model.transformer.layers.[17-17].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.17" -> "NoTPAttention @ vision_model.transformer.layers.[17-17].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.17.self_attn" -> "Linear @ vision_model.transformer.layers.[17-17].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.17.self_attn" -> "Linear @ vision_model.transformer.layers.[17-17].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.17" -> "LayerNorm @ vision_model.transformer.layers.[17-17].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.17" -> "NoTPFeedForward @ vision_model.transformer.layers.[17-17].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.17.mlp" -> "Linear @ vision_model.transformer.layers.[17-17].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.17.mlp" -> "Linear @ vision_model.transformer.layers.[17-17].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.18" -> "LayerNorm @ vision_model.transformer.layers.[18-18].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.18" -> "NoTPAttention @ vision_model.transformer.layers.[18-18].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.18.self_attn" -> "Linear @ vision_model.transformer.layers.[18-18].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.18.self_attn" -> "Linear @ vision_model.transformer.layers.[18-18].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.18" -> "LayerNorm @ vision_model.transformer.layers.[18-18].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.18" -> "NoTPFeedForward @ vision_model.transformer.layers.[18-18].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.18.mlp" -> "Linear @ vision_model.transformer.layers.[18-18].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.18.mlp" -> "Linear @ vision_model.transformer.layers.[18-18].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.19" -> "LayerNorm @ vision_model.transformer.layers.[19-19].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.19" -> "NoTPAttention @ vision_model.transformer.layers.[19-19].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.19.self_attn" -> "Linear @ vision_model.transformer.layers.[19-19].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.19.self_attn" -> "Linear @ vision_model.transformer.layers.[19-19].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.19" -> "LayerNorm @ vision_model.transformer.layers.[19-19].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.19" -> "NoTPFeedForward @ vision_model.transformer.layers.[19-19].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.19.mlp" -> "Linear @ vision_model.transformer.layers.[19-19].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.19.mlp" -> "Linear @ vision_model.transformer.layers.[19-19].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.20" -> "LayerNorm @ vision_model.transformer.layers.[20-20].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.20" -> "NoTPAttention @ vision_model.transformer.layers.[20-20].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.20.self_attn" -> "Linear @ vision_model.transformer.layers.[20-20].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.20.self_attn" -> "Linear @ vision_model.transformer.layers.[20-20].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.20" -> "LayerNorm @ vision_model.transformer.layers.[20-20].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.20" -> "NoTPFeedForward @ vision_model.transformer.layers.[20-20].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.20.mlp" -> "Linear @ vision_model.transformer.layers.[20-20].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.20.mlp" -> "Linear @ vision_model.transformer.layers.[20-20].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.21" -> "LayerNorm @ vision_model.transformer.layers.[21-21].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.21" -> "NoTPAttention @ vision_model.transformer.layers.[21-21].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.21.self_attn" -> "Linear @ vision_model.transformer.layers.[21-21].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.21.self_attn" -> "Linear @ vision_model.transformer.layers.[21-21].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.21" -> "LayerNorm @ vision_model.transformer.layers.[21-21].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.21" -> "NoTPFeedForward @ vision_model.transformer.layers.[21-21].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.21.mlp" -> "Linear @ vision_model.transformer.layers.[21-21].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.21.mlp" -> "Linear @ vision_model.transformer.layers.[21-21].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.22" -> "LayerNorm @ vision_model.transformer.layers.[22-22].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.22" -> "NoTPAttention @ vision_model.transformer.layers.[22-22].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.22.self_attn" -> "Linear @ vision_model.transformer.layers.[22-22].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.22.self_attn" -> "Linear @ vision_model.transformer.layers.[22-22].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.22" -> "LayerNorm @ vision_model.transformer.layers.[22-22].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.22" -> "NoTPFeedForward @ vision_model.transformer.layers.[22-22].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.22.mlp" -> "Linear @ vision_model.transformer.layers.[22-22].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.22.mlp" -> "Linear @ vision_model.transformer.layers.[22-22].mlp.fc2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.23" -> "LayerNorm @ vision_model.transformer.layers.[23-23].layer_norm1" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.23" -> "NoTPAttention @ vision_model.transformer.layers.[23-23].self_attn" [label="parfor 30"];
  "NoTPAttention @ vision_model.transformer.layers.23.self_attn" -> "Linear @ vision_model.transformer.layers.[23-23].self_attn.qkv_proj" [label="parfor 2"];
  "NoTPAttention @ vision_model.transformer.layers.23.self_attn" -> "Linear @ vision_model.transformer.layers.[23-23].self_attn.out_proj" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.23" -> "LayerNorm @ vision_model.transformer.layers.[23-23].layer_norm2" [label="parfor 2"];
  "NoTPTransformerBlock @ vision_model.transformer.layers.23" -> "NoTPFeedForward @ vision_model.transformer.layers.[23-23].mlp" [label="parfor 10"];
  "NoTPFeedForward @ vision_model.transformer.layers.23.mlp" -> "Linear @ vision_model.transformer.layers.[23-23].mlp.fc1" [label="parfor 2"];
  "NoTPFeedForward @ vision_model.transformer.layers.23.mlp" -> "Linear @ vision_model.transformer.layers.[23-23].mlp.fc2" [label="parfor 2"];
  "DeepseekV2DecoderLayer @ layers.0" -> "DeepseekV2RMSNorm @ layers.[0-0].input_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.0" -> "LlamaAttention @ layers.[0-0].self_attn" [label="parfor 55"];
  "LlamaAttention @ layers.0.self_attn" -> "LlamaRotaryEmbedding @ layers.[0-0].self_attn.rotary_emb" [label="parfor 17"];
  "DeepseekV2DecoderLayer @ layers.0" -> "DeepseekV2RMSNorm @ layers.[0-0].post_attention_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.0" -> "DeepseekV2MLP @ layers.[0-0].mlp" [label="parfor 5"];
  "DeepseekV2DecoderLayer @ layers.1" -> "DeepseekV2RMSNorm @ layers.[1-1].input_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.1" -> "LlamaAttention @ layers.[1-1].self_attn" [label="parfor 57"];
  "LlamaAttention @ layers.1.self_attn" -> "LlamaRotaryEmbedding @ layers.[1-1].self_attn.rotary_emb" [label="parfor 17"];
  "DeepseekV2DecoderLayer @ layers.1" -> "DeepseekV2RMSNorm @ layers.[1-1].post_attention_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.1" -> "DeepseekV2MoE @ layers.[1-1].mlp" [label="parfor 418"];
  "DeepseekV2MoE @ layers.1.mlp" -> "MoEGate @ layers.[1-1].mlp.gate" [label="parfor 8"];
  "DeepseekV2MoE @ layers.1.mlp" -> "DeepseekV2MLP @ layers.1.mlp.experts.[0-63]" [label="parfor 320"];
  "DeepseekV2MoE @ layers.1.mlp" -> "DeepseekV2MLP @ layers.[1-1].mlp.shared_experts" [label="parfor 5"];
  "DeepseekV2DecoderLayer @ layers.2" -> "DeepseekV2RMSNorm @ layers.[2-2].input_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.2" -> "LlamaAttention @ layers.[2-2].self_attn" [label="parfor 59"];
  "LlamaAttention @ layers.2.self_attn" -> "LlamaRotaryEmbedding @ layers.[2-2].self_attn.rotary_emb" [label="parfor 17"];
  "DeepseekV2DecoderLayer @ layers.2" -> "DeepseekV2RMSNorm @ layers.[2-2].post_attention_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.2" -> "DeepseekV2MoE @ layers.[2-2].mlp" [label="parfor 406"];
  "DeepseekV2MoE @ layers.2.mlp" -> "MoEGate @ layers.[2-2].mlp.gate" [label="parfor 8"];
  "DeepseekV2MoE @ layers.2.mlp" -> "DeepseekV2MLP @ layers.2.mlp.experts.[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,59,60,61,62,63]" [label="parfor 310"];
  "DeepseekV2MoE @ layers.2.mlp" -> "DeepseekV2MLP @ layers.[2-2].mlp.shared_experts" [label="parfor 5"];
  "DeepseekV2DecoderLayer @ layers.3" -> "DeepseekV2RMSNorm @ layers.[3-3].input_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.3" -> "LlamaAttention @ layers.[3-3].self_attn" [label="parfor 61"];
  "LlamaAttention @ layers.3.self_attn" -> "LlamaRotaryEmbedding @ layers.[3-3].self_attn.rotary_emb" [label="parfor 17"];
  "DeepseekV2DecoderLayer @ layers.3" -> "DeepseekV2RMSNorm @ layers.[3-3].post_attention_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.3" -> "DeepseekV2MoE @ layers.[3-3].mlp" [label="parfor 412"];
  "DeepseekV2MoE @ layers.3.mlp" -> "MoEGate @ layers.[3-3].mlp.gate" [label="parfor 8"];
  "DeepseekV2MoE @ layers.3.mlp" -> "DeepseekV2MLP @ layers.3.mlp.experts.[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,54,55,56,57,58,59,60,61,62,63]" [label="parfor 315"];
  "DeepseekV2MoE @ layers.3.mlp" -> "DeepseekV2MLP @ layers.[3-3].mlp.shared_experts" [label="parfor 5"];
  "DeepseekV2DecoderLayer @ layers.4" -> "DeepseekV2RMSNorm @ layers.[4-4].input_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.4" -> "LlamaAttention @ layers.[4-4].self_attn" [label="parfor 63"];
  "LlamaAttention @ layers.4.self_attn" -> "LlamaRotaryEmbedding @ layers.[4-4].self_attn.rotary_emb" [label="parfor 17"];
  "DeepseekV2DecoderLayer @ layers.4" -> "DeepseekV2RMSNorm @ layers.[4-4].post_attention_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.4" -> "DeepseekV2MoE @ layers.[4-4].mlp" [label="parfor 418"];
  "DeepseekV2MoE @ layers.4.mlp" -> "MoEGate @ layers.[4-4].mlp.gate" [label="parfor 8"];
  "DeepseekV2MoE @ layers.4.mlp" -> "DeepseekV2MLP @ layers.4.mlp.experts.[0-63]" [label="parfor 320"];
  "DeepseekV2MoE @ layers.4.mlp" -> "DeepseekV2MLP @ layers.[4-4].mlp.shared_experts" [label="parfor 5"];
  "DeepseekV2DecoderLayer @ layers.5" -> "DeepseekV2RMSNorm @ layers.[5-5].input_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.5" -> "LlamaAttention @ layers.[5-5].self_attn" [label="parfor 65"];
  "LlamaAttention @ layers.5.self_attn" -> "LlamaRotaryEmbedding @ layers.[5-5].self_attn.rotary_emb" [label="parfor 17"];
  "DeepseekV2DecoderLayer @ layers.5" -> "DeepseekV2RMSNorm @ layers.[5-5].post_attention_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.5" -> "DeepseekV2MoE @ layers.[5-5].mlp" [label="parfor 418"];
  "DeepseekV2MoE @ layers.5.mlp" -> "MoEGate @ layers.[5-5].mlp.gate" [label="parfor 8"];
  "DeepseekV2MoE @ layers.5.mlp" -> "DeepseekV2MLP @ layers.5.mlp.experts.[0-63]" [label="parfor 320"];
  "DeepseekV2MoE @ layers.5.mlp" -> "DeepseekV2MLP @ layers.[5-5].mlp.shared_experts" [label="parfor 5"];
  "DeepseekV2DecoderLayer @ layers.6" -> "DeepseekV2RMSNorm @ layers.[6-6].input_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.6" -> "LlamaAttention @ layers.[6-6].self_attn" [label="parfor 67"];
  "LlamaAttention @ layers.6.self_attn" -> "LlamaRotaryEmbedding @ layers.[6-6].self_attn.rotary_emb" [label="parfor 17"];
  "DeepseekV2DecoderLayer @ layers.6" -> "DeepseekV2RMSNorm @ layers.[6-6].post_attention_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.6" -> "DeepseekV2MoE @ layers.[6-6].mlp" [label="parfor 418"];
  "DeepseekV2MoE @ layers.6.mlp" -> "MoEGate @ layers.[6-6].mlp.gate" [label="parfor 8"];
  "DeepseekV2MoE @ layers.6.mlp" -> "DeepseekV2MLP @ layers.6.mlp.experts.[0-63]" [label="parfor 320"];
  "DeepseekV2MoE @ layers.6.mlp" -> "DeepseekV2MLP @ layers.[6-6].mlp.shared_experts" [label="parfor 5"];
  "DeepseekV2DecoderLayer @ layers.7" -> "DeepseekV2RMSNorm @ layers.[7-7].input_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.7" -> "LlamaAttention @ layers.[7-7].self_attn" [label="parfor 69"];
  "LlamaAttention @ layers.7.self_attn" -> "LlamaRotaryEmbedding @ layers.[7-7].self_attn.rotary_emb" [label="parfor 17"];
  "DeepseekV2DecoderLayer @ layers.7" -> "DeepseekV2RMSNorm @ layers.[7-7].post_attention_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.7" -> "DeepseekV2MoE @ layers.[7-7].mlp" [label="parfor 418"];
  "DeepseekV2MoE @ layers.7.mlp" -> "MoEGate @ layers.[7-7].mlp.gate" [label="parfor 8"];
  "DeepseekV2MoE @ layers.7.mlp" -> "DeepseekV2MLP @ layers.7.mlp.experts.[0-63]" [label="parfor 320"];
  "DeepseekV2MoE @ layers.7.mlp" -> "DeepseekV2MLP @ layers.[7-7].mlp.shared_experts" [label="parfor 5"];
  "DeepseekV2DecoderLayer @ layers.8" -> "DeepseekV2RMSNorm @ layers.[8-8].input_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.8" -> "LlamaAttention @ layers.[8-8].self_attn" [label="parfor 71"];
  "LlamaAttention @ layers.8.self_attn" -> "LlamaRotaryEmbedding @ layers.[8-8].self_attn.rotary_emb" [label="parfor 17"];
  "DeepseekV2DecoderLayer @ layers.8" -> "DeepseekV2RMSNorm @ layers.[8-8].post_attention_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.8" -> "DeepseekV2MoE @ layers.[8-8].mlp" [label="parfor 412"];
  "DeepseekV2MoE @ layers.8.mlp" -> "MoEGate @ layers.[8-8].mlp.gate" [label="parfor 8"];
  "DeepseekV2MoE @ layers.8.mlp" -> "DeepseekV2MLP @ layers.8.mlp.experts.[0,1,2,3,4,5,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63]" [label="parfor 315"];
  "DeepseekV2MoE @ layers.8.mlp" -> "DeepseekV2MLP @ layers.[8-8].mlp.shared_experts" [label="parfor 5"];
  "DeepseekV2DecoderLayer @ layers.9" -> "DeepseekV2RMSNorm @ layers.[9-9].input_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.9" -> "LlamaAttention @ layers.[9-9].self_attn" [label="parfor 73"];
  "LlamaAttention @ layers.9.self_attn" -> "LlamaRotaryEmbedding @ layers.[9-9].self_attn.rotary_emb" [label="parfor 17"];
  "DeepseekV2DecoderLayer @ layers.9" -> "DeepseekV2RMSNorm @ layers.[9-9].post_attention_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.9" -> "DeepseekV2MoE @ layers.[9-9].mlp" [label="parfor 418"];
  "DeepseekV2MoE @ layers.9.mlp" -> "MoEGate @ layers.[9-9].mlp.gate" [label="parfor 8"];
  "DeepseekV2MoE @ layers.9.mlp" -> "DeepseekV2MLP @ layers.9.mlp.experts.[0-63]" [label="parfor 320"];
  "DeepseekV2MoE @ layers.9.mlp" -> "DeepseekV2MLP @ layers.[9-9].mlp.shared_experts" [label="parfor 5"];
  "DeepseekV2DecoderLayer @ layers.10" -> "DeepseekV2RMSNorm @ layers.[10-10].input_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.10" -> "LlamaAttention @ layers.[10-10].self_attn" [label="parfor 75"];
  "LlamaAttention @ layers.10.self_attn" -> "LlamaRotaryEmbedding @ layers.[10-10].self_attn.rotary_emb" [label="parfor 17"];
  "DeepseekV2DecoderLayer @ layers.10" -> "DeepseekV2RMSNorm @ layers.[10-10].post_attention_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.10" -> "DeepseekV2MoE @ layers.[10-10].mlp" [label="parfor 418"];
  "DeepseekV2MoE @ layers.10.mlp" -> "MoEGate @ layers.[10-10].mlp.gate" [label="parfor 8"];
  "DeepseekV2MoE @ layers.10.mlp" -> "DeepseekV2MLP @ layers.10.mlp.experts.[0-63]" [label="parfor 320"];
  "DeepseekV2MoE @ layers.10.mlp" -> "DeepseekV2MLP @ layers.[10-10].mlp.shared_experts" [label="parfor 5"];
  "DeepseekV2DecoderLayer @ layers.11" -> "DeepseekV2RMSNorm @ layers.[11-11].input_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.11" -> "LlamaAttention @ layers.[11-11].self_attn" [label="parfor 77"];
  "LlamaAttention @ layers.11.self_attn" -> "LlamaRotaryEmbedding @ layers.[11-11].self_attn.rotary_emb" [label="parfor 17"];
  "DeepseekV2DecoderLayer @ layers.11" -> "DeepseekV2RMSNorm @ layers.[11-11].post_attention_layernorm" [label="parfor 8"];
  "DeepseekV2DecoderLayer @ layers.11" -> "DeepseekV2MoE @ layers.[11-11].mlp" [label="parfor 418"];
  "DeepseekV2MoE @ layers.11.mlp" -> "MoEGate @ layers.[11-11].mlp.gate" [label="parfor 8"];
  "DeepseekV2MoE @ layers.11.mlp" -> "DeepseekV2MLP @ layers.11.mlp.experts.[0-63]" [label="parfor 320"];
  "DeepseekV2MoE @ layers.11.mlp" -> "DeepseekV2MLP @ layers.[11-11].mlp.shared_experts" [label="parfor 5"];
  "ImageEncoderViT @ sam_model" -> "PatchEmbed @ sam_model.patch_embed" [label="parfor 4"];
  "PatchEmbed @ sam_model.patch_embed" -> "Conv2d @ sam_model.patch_embed.proj" [label="parfor 2"];
  "ImageEncoderViT @ sam_model" -> "Sequential @ sam_model.neck" [label="parfor 52"];
  "ImageEncoderViT @ sam_model" -> "Conv2d @ sam_model.net_2" [label="parfor 2"];
  "ImageEncoderViT @ sam_model" -> "Conv2d @ sam_model.net_3" [label="parfor 2"];
  "VitModel @ vision_model" -> "CLIPVisionEmbeddings @ vision_model.embeddings" [label="parfor 30"];
  "CLIPVisionEmbeddings @ vision_model.embeddings" -> "Embedding @ vision_model.embeddings.position_embedding" [label="parfor 4"];
  "VitModel @ vision_model" -> "LayerNorm @ vision_model.pre_layrnorm" [label="parfor 2"];
  "VitModel @ vision_model" -> "NoTPTransformer @ vision_model.transformer" [label="parfor 1152"];
  "MlpProjector @ projector" -> "Linear @ projector.layers" [label="parfor 2"];
}