digraph dsocr_grouped {
  rankdir=LR;
  "Block @ sam_model.blocks[0..11]" [label="Block @ sam_model.blocks[0..11]\nfor 12\nstage=sam\nparent=sam_model", shape=box];
  "Conv2d @ sam_model.neck[0..3]" [label="Conv2d @ sam_model.neck[0..3]\nfor 4\nstage=sam\nparent=sam_model.neck", shape=box];
  "NoTPTransformerBlock @ vision_model.transformer.layers[0..23]" [label="NoTPTransformerBlock @ vision_model.transformer.layers[0..23]\nfor 24\nstage=vision\nparent=vision_model.transformer", shape=box];
  "DeepseekV2DecoderLayer @ layers[0..11]" [label="DeepseekV2DecoderLayer @ layers[0..11]\nfor 12", shape=box];
  "DeepseekV2MLP @ layers.1.mlp.experts[0..63]" [label="DeepseekV2MLP @ layers.1.mlp.experts[0..63]\nfor 64\nstage=llm\nparent=layers.1.mlp", shape=box];
  "DeepseekV2MLP @ layers.2.mlp.experts[0..18]" [label="DeepseekV2MLP @ layers.2.mlp.experts[0..18]\nfor 19\nstage=llm\nparent=layers.2.mlp", shape=box];
  "DeepseekV2MLP @ layers.2.mlp.experts[20..57]" [label="DeepseekV2MLP @ layers.2.mlp.experts[20..57]\nfor 38\nstage=llm\nparent=layers.2.mlp", shape=box];
  "DeepseekV2MLP @ layers.2.mlp.experts[59..63]" [label="DeepseekV2MLP @ layers.2.mlp.experts[59..63]\nfor 5\nstage=llm\nparent=layers.2.mlp", shape=box];
  "DeepseekV2MLP @ layers.3.mlp.experts[0..52]" [label="DeepseekV2MLP @ layers.3.mlp.experts[0..52]\nfor 53\nstage=llm\nparent=layers.3.mlp", shape=box];
  "DeepseekV2MLP @ layers.3.mlp.experts[54..63]" [label="DeepseekV2MLP @ layers.3.mlp.experts[54..63]\nfor 10\nstage=llm\nparent=layers.3.mlp", shape=box];
  "DeepseekV2MLP @ layers.4.mlp.experts[0..63]" [label="DeepseekV2MLP @ layers.4.mlp.experts[0..63]\nfor 64\nstage=llm\nparent=layers.4.mlp", shape=box];
  "DeepseekV2MLP @ layers.5.mlp.experts[0..63]" [label="DeepseekV2MLP @ layers.5.mlp.experts[0..63]\nfor 64\nstage=llm\nparent=layers.5.mlp", shape=box];
  "DeepseekV2MLP @ layers.6.mlp.experts[0..63]" [label="DeepseekV2MLP @ layers.6.mlp.experts[0..63]\nfor 64\nstage=llm\nparent=layers.6.mlp", shape=box];
  "DeepseekV2MLP @ layers.7.mlp.experts[0..63]" [label="DeepseekV2MLP @ layers.7.mlp.experts[0..63]\nfor 64\nstage=llm\nparent=layers.7.mlp", shape=box];
  "DeepseekV2MLP @ layers.8.mlp.experts[0..5]" [label="DeepseekV2MLP @ layers.8.mlp.experts[0..5]\nfor 6\nstage=llm\nparent=layers.8.mlp", shape=box];
  "DeepseekV2MLP @ layers.8.mlp.experts[7..63]" [label="DeepseekV2MLP @ layers.8.mlp.experts[7..63]\nfor 57\nstage=llm\nparent=layers.8.mlp", shape=box];
  "DeepseekV2MLP @ layers.9.mlp.experts[0..63]" [label="DeepseekV2MLP @ layers.9.mlp.experts[0..63]\nfor 64\nstage=llm\nparent=layers.9.mlp", shape=box];
  "DeepseekV2MLP @ layers.10.mlp.experts[0..63]" [label="DeepseekV2MLP @ layers.10.mlp.experts[0..63]\nfor 64\nstage=llm\nparent=layers.10.mlp", shape=box];
  "DeepseekV2MLP @ layers.11.mlp.experts[0..63]" [label="DeepseekV2MLP @ layers.11.mlp.experts[0..63]\nfor 64\nstage=llm\nparent=layers.11.mlp", shape=box];
  "PatchEmbed @ sam_model.patch_embed" [label="PatchEmbed @ sam_model.patch_embed\nparfor 4\nstage=sam\nparent=sam_model", shape=box];
  "Conv2d @ sam_model.patch_embed.proj" [label="Conv2d @ sam_model.patch_embed.proj\nparfor 2\nstage=sam\nparent=sam_model.patch_embed", shape=box];
  "LayerNorm @ sam_model.blocks.*.norm1" [label="LayerNorm @ sam_model.blocks.*.norm1\nparfor 24\nstage=sam\nparent=sam_model.blocks.*", shape=box];
  "Attention @ sam_model.blocks.*.attn" [label="Attention @ sam_model.blocks.*.attn\nparfor 1160\nstage=sam\nparent=sam_model.blocks.*", shape=box];
  "Linear @ sam_model.blocks.*.attn.qkv" [label="Linear @ sam_model.blocks.*.attn.qkv\nparfor 24\nstage=sam\nparent=sam_model.blocks.*.attn", shape=box];
  "Linear @ sam_model.blocks.*.attn.proj" [label="Linear @ sam_model.blocks.*.attn.proj\nparfor 24\nstage=sam\nparent=sam_model.blocks.*.attn", shape=box];
  "LayerNorm @ sam_model.blocks.*.norm2" [label="LayerNorm @ sam_model.blocks.*.norm2\nparfor 24\nstage=sam\nparent=sam_model.blocks.*", shape=box];
  "MLPBlock @ sam_model.blocks.*.mlp" [label="MLPBlock @ sam_model.blocks.*.mlp\nparfor 72\nstage=sam\nparent=sam_model.blocks.*", shape=box];
  "Linear @ sam_model.blocks.*.mlp.lin1" [label="Linear @ sam_model.blocks.*.mlp.lin1\nparfor 24\nstage=sam\nparent=sam_model.blocks.*.mlp", shape=box];
  "GELU @ sam_model.blocks.*.mlp.act" [label="GELU @ sam_model.blocks.*.mlp.act\nparfor 24\nstage=sam\nparent=sam_model.blocks.*.mlp", shape=box];
  "Linear @ sam_model.blocks.*.mlp.lin2" [label="Linear @ sam_model.blocks.*.mlp.lin2\nparfor 24\nstage=sam\nparent=sam_model.blocks.*.mlp", shape=box];
  "Conv2d @ sam_model.net_2" [label="Conv2d @ sam_model.net_2\nparfor 2\nstage=sam\nparent=sam_model", shape=box];
  "Conv2d @ sam_model.net_3" [label="Conv2d @ sam_model.net_3\nparfor 2\nstage=sam\nparent=sam_model", shape=box];
  "CLIPVisionEmbeddings @ vision_model.embeddings" [label="CLIPVisionEmbeddings @ vision_model.embeddings\nparfor 30\nstage=vision\nparent=vision_model", shape=box];
  "Embedding @ vision_model.embeddings.position_embedding" [label="Embedding @ vision_model.embeddings.position_embedding\nparfor 4\nstage=vision\nparent=vision_model.embeddings", shape=box];
  "LayerNorm @ vision_model.pre_layrnorm" [label="LayerNorm @ vision_model.pre_layrnorm\nparfor 2\nstage=vision\nparent=vision_model", shape=box];
  "NoTPTransformer @ vision_model.transformer" [label="NoTPTransformer @ vision_model.transformer\nparfor 1152\nstage=vision\nparent=vision_model", shape=box];
  "LayerNorm @ vision_model.transformer.layers.*.layer_norm1" [label="LayerNorm @ vision_model.transformer.layers.*.layer_norm1\nparfor 48\nstage=vision\nparent=vision_model.transformer.layers.*", shape=box];
  "NoTPAttention @ vision_model.transformer.layers.*.self_attn" [label="NoTPAttention @ vision_model.transformer.layers.*.self_attn\nparfor 720\nstage=vision\nparent=vision_model.transformer.layers.*", shape=box];
  "Linear @ vision_model.transformer.layers.*.self_attn.qkv_proj" [label="Linear @ vision_model.transformer.layers.*.self_attn.qkv_proj\nparfor 48\nstage=vision\nparent=vision_model.transformer.layers.*.self_attn", shape=box];
  "Linear @ vision_model.transformer.layers.*.self_attn.out_proj" [label="Linear @ vision_model.transformer.layers.*.self_attn.out_proj\nparfor 48\nstage=vision\nparent=vision_model.transformer.layers.*.self_attn", shape=box];
  "LayerNorm @ vision_model.transformer.layers.*.layer_norm2" [label="LayerNorm @ vision_model.transformer.layers.*.layer_norm2\nparfor 48\nstage=vision\nparent=vision_model.transformer.layers.*", shape=box];
  "NoTPFeedForward @ vision_model.transformer.layers.*.mlp" [label="NoTPFeedForward @ vision_model.transformer.layers.*.mlp\nparfor 240\nstage=vision\nparent=vision_model.transformer.layers.*", shape=box];
  "Linear @ vision_model.transformer.layers.*.mlp.fc1" [label="Linear @ vision_model.transformer.layers.*.mlp.fc1\nparfor 48\nstage=vision\nparent=vision_model.transformer.layers.*.mlp", shape=box];
  "Linear @ vision_model.transformer.layers.*.mlp.fc2" [label="Linear @ vision_model.transformer.layers.*.mlp.fc2\nparfor 48\nstage=vision\nparent=vision_model.transformer.layers.*.mlp", shape=box];
  "Linear @ projector.layers" [label="Linear @ projector.layers\nparfor 2\nparent=projector", shape=box];
  "DeepseekV2RMSNorm @ layers.*.input_layernorm" [label="DeepseekV2RMSNorm @ layers.*.input_layernorm\nparfor 96\nstage=llm\nparent=layers.*", shape=box];
  "LlamaAttention @ layers.*.self_attn" [label="LlamaAttention @ layers.*.self_attn\nparfor 792\nstage=llm\nparent=layers.*", shape=box];
  "LlamaRotaryEmbedding @ layers.*.self_attn.rotary_emb" [label="LlamaRotaryEmbedding @ layers.*.self_attn.rotary_emb\nparfor 204\nstage=llm\nparent=layers.*.self_attn", shape=box];
  "DeepseekV2RMSNorm @ layers.*.post_attention_layernorm" [label="DeepseekV2RMSNorm @ layers.*.post_attention_layernorm\nparfor 96\nstage=llm\nparent=layers.*", shape=box];
  "DeepseekV2MLP @ layers.*.mlp" [label="DeepseekV2MLP @ layers.*.mlp\nparfor 4579\nstage=llm\nparent=layers.*", shape=box];
  "MoEGate @ layers.*.mlp.gate" [label="MoEGate @ layers.*.mlp.gate\nparfor 88\nstage=llm\nparent=layers.*.mlp", shape=box];
  "DeepseekV2MLP @ layers.*.mlp.shared_experts" [label="DeepseekV2MLP @ layers.*.mlp.shared_experts\nparfor 55\nstage=llm\nparent=layers.*.mlp", shape=box];
  "sam_model" [label="sam_model"];
  "sam_model" -> "Block @ sam_model.blocks[0..11]" [label="for 12"];
  "sam_model.neck" [label="sam_model.neck"];
  "sam_model.neck" -> "Conv2d @ sam_model.neck[0..3]" [label="for 4"];
  "vision_model.transformer" [label="vision_model.transformer"];
  "vision_model.transformer" -> "NoTPTransformerBlock @ vision_model.transformer.layers[0..23]" [label="for 24"];
  "layers.1.mlp" [label="layers.1.mlp"];
  "layers.1.mlp" -> "DeepseekV2MLP @ layers.1.mlp.experts[0..63]" [label="for 64"];
  "layers.2.mlp" [label="layers.2.mlp"];
  "layers.2.mlp" -> "DeepseekV2MLP @ layers.2.mlp.experts[0..18]" [label="for 19"];
  "layers.2.mlp" -> "DeepseekV2MLP @ layers.2.mlp.experts[20..57]" [label="for 38"];
  "layers.2.mlp" -> "DeepseekV2MLP @ layers.2.mlp.experts[59..63]" [label="for 5"];
  "layers.3.mlp" [label="layers.3.mlp"];
  "layers.3.mlp" -> "DeepseekV2MLP @ layers.3.mlp.experts[0..52]" [label="for 53"];
  "layers.3.mlp" -> "DeepseekV2MLP @ layers.3.mlp.experts[54..63]" [label="for 10"];
  "layers.4.mlp" [label="layers.4.mlp"];
  "layers.4.mlp" -> "DeepseekV2MLP @ layers.4.mlp.experts[0..63]" [label="for 64"];
  "layers.5.mlp" [label="layers.5.mlp"];
  "layers.5.mlp" -> "DeepseekV2MLP @ layers.5.mlp.experts[0..63]" [label="for 64"];
  "layers.6.mlp" [label="layers.6.mlp"];
  "layers.6.mlp" -> "DeepseekV2MLP @ layers.6.mlp.experts[0..63]" [label="for 64"];
  "layers.7.mlp" [label="layers.7.mlp"];
  "layers.7.mlp" -> "DeepseekV2MLP @ layers.7.mlp.experts[0..63]" [label="for 64"];
  "layers.8.mlp" [label="layers.8.mlp"];
  "layers.8.mlp" -> "DeepseekV2MLP @ layers.8.mlp.experts[0..5]" [label="for 6"];
  "layers.8.mlp" -> "DeepseekV2MLP @ layers.8.mlp.experts[7..63]" [label="for 57"];
  "layers.9.mlp" [label="layers.9.mlp"];
  "layers.9.mlp" -> "DeepseekV2MLP @ layers.9.mlp.experts[0..63]" [label="for 64"];
  "layers.10.mlp" [label="layers.10.mlp"];
  "layers.10.mlp" -> "DeepseekV2MLP @ layers.10.mlp.experts[0..63]" [label="for 64"];
  "layers.11.mlp" [label="layers.11.mlp"];
  "layers.11.mlp" -> "DeepseekV2MLP @ layers.11.mlp.experts[0..63]" [label="for 64"];
  "sam_model" -> "PatchEmbed @ sam_model.patch_embed" [label="parfor 4"];
  "sam_model.patch_embed" [label="sam_model.patch_embed"];
  "sam_model.patch_embed" -> "Conv2d @ sam_model.patch_embed.proj" [label="parfor 2"];
  "sam_model.blocks.*" [label="sam_model.blocks.*"];
  "sam_model.blocks.*" -> "LayerNorm @ sam_model.blocks.*.norm1" [label="parfor 24"];
  "sam_model.blocks.*" -> "Attention @ sam_model.blocks.*.attn" [label="parfor 1160"];
  "sam_model.blocks.*.attn" [label="sam_model.blocks.*.attn"];
  "sam_model.blocks.*.attn" -> "Linear @ sam_model.blocks.*.attn.qkv" [label="parfor 24"];
  "sam_model.blocks.*.attn" -> "Linear @ sam_model.blocks.*.attn.proj" [label="parfor 24"];
  "sam_model.blocks.*" -> "LayerNorm @ sam_model.blocks.*.norm2" [label="parfor 24"];
  "sam_model.blocks.*" -> "MLPBlock @ sam_model.blocks.*.mlp" [label="parfor 72"];
  "sam_model.blocks.*.mlp" [label="sam_model.blocks.*.mlp"];
  "sam_model.blocks.*.mlp" -> "Linear @ sam_model.blocks.*.mlp.lin1" [label="parfor 24"];
  "sam_model.blocks.*.mlp" -> "GELU @ sam_model.blocks.*.mlp.act" [label="parfor 24"];
  "sam_model.blocks.*.mlp" -> "Linear @ sam_model.blocks.*.mlp.lin2" [label="parfor 24"];
  "sam_model" -> "Conv2d @ sam_model.net_2" [label="parfor 2"];
  "sam_model" -> "Conv2d @ sam_model.net_3" [label="parfor 2"];
  "vision_model" [label="vision_model"];
  "vision_model" -> "CLIPVisionEmbeddings @ vision_model.embeddings" [label="parfor 30"];
  "vision_model.embeddings" [label="vision_model.embeddings"];
  "vision_model.embeddings" -> "Embedding @ vision_model.embeddings.position_embedding" [label="parfor 4"];
  "vision_model" -> "LayerNorm @ vision_model.pre_layrnorm" [label="parfor 2"];
  "vision_model" -> "NoTPTransformer @ vision_model.transformer" [label="parfor 1152"];
  "vision_model.transformer.layers.*" [label="vision_model.transformer.layers.*"];
  "vision_model.transformer.layers.*" -> "LayerNorm @ vision_model.transformer.layers.*.layer_norm1" [label="parfor 48"];
  "vision_model.transformer.layers.*" -> "NoTPAttention @ vision_model.transformer.layers.*.self_attn" [label="parfor 720"];
  "vision_model.transformer.layers.*.self_attn" [label="vision_model.transformer.layers.*.self_attn"];
  "vision_model.transformer.layers.*.self_attn" -> "Linear @ vision_model.transformer.layers.*.self_attn.qkv_proj" [label="parfor 48"];
  "vision_model.transformer.layers.*.self_attn" -> "Linear @ vision_model.transformer.layers.*.self_attn.out_proj" [label="parfor 48"];
  "vision_model.transformer.layers.*" -> "LayerNorm @ vision_model.transformer.layers.*.layer_norm2" [label="parfor 48"];
  "vision_model.transformer.layers.*" -> "NoTPFeedForward @ vision_model.transformer.layers.*.mlp" [label="parfor 240"];
  "vision_model.transformer.layers.*.mlp" [label="vision_model.transformer.layers.*.mlp"];
  "vision_model.transformer.layers.*.mlp" -> "Linear @ vision_model.transformer.layers.*.mlp.fc1" [label="parfor 48"];
  "vision_model.transformer.layers.*.mlp" -> "Linear @ vision_model.transformer.layers.*.mlp.fc2" [label="parfor 48"];
  "projector" [label="projector"];
  "projector" -> "Linear @ projector.layers" [label="parfor 2"];
  "layers.*" [label="layers.*"];
  "layers.*" -> "DeepseekV2RMSNorm @ layers.*.input_layernorm" [label="parfor 96"];
  "layers.*" -> "LlamaAttention @ layers.*.self_attn" [label="parfor 792"];
  "layers.*.self_attn" [label="layers.*.self_attn"];
  "layers.*.self_attn" -> "LlamaRotaryEmbedding @ layers.*.self_attn.rotary_emb" [label="parfor 204"];
  "layers.*" -> "DeepseekV2RMSNorm @ layers.*.post_attention_layernorm" [label="parfor 96"];
  "layers.*" -> "DeepseekV2MLP @ layers.*.mlp" [label="parfor 4579"];
  "layers.*.mlp" [label="layers.*.mlp"];
  "layers.*.mlp" -> "MoEGate @ layers.*.mlp.gate" [label="parfor 88"];
  "layers.*.mlp" -> "DeepseekV2MLP @ layers.*.mlp.shared_experts" [label="parfor 55"];
}