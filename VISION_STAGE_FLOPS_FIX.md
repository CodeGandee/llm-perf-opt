# How to Compute Vision Stage FLOPs - Implementation Guide

**Date**: 2025-10-29
**Issue Resolved**: Vision stages (SAM, CLIP, projector) showing 0 FLOPs
**Root Cause**: Module path and input dimension mismatches

---

## Problem Summary

The initial implementation had vision stages showing 0 FLOPs because:

1. **Module Path Issue**: Vision modules are nested under `.model` attribute
   - Incorrect: `session.m_model.sam_model`
   - Correct: `session.m_model.model.sam_model`

2. **Full Model Tracing Issue**: fvcore JIT tracing requires positional arguments
   - Model forward needs `images`, `images_seq_mask`, `images_spatial_crop` as kwargs
   - fvcore's FlopCountAnalysis only passes positional args
   - Solution: Create wrapper that converts kwargs to positional args

3. **CLIP Input Mismatch**: CLIP needs SAM features with correct batch dimensions
   - Initial: Using mock features or crop SAM features (batch=4)
   - Fixed: Run SAM on global view to get real features (batch=1)

---

## Fixes Applied

### Fix 1: Correct Module Access Paths

Updated all isolated analysis methods to use nested model structure:

```python
# Before
sam_model = getattr(self.m_session.m_model, "sam_model", None)

# After
base_model = getattr(self.m_session.m_model, "model", self.m_session.m_model)
sam_model = getattr(base_model, "sam_model", None)
```

Applied to: SAM, CLIP, and projector isolated analyses.

### Fix 2: Update Stage-Module Mapping

Updated the extraction mapping to match nested structure:

```python
# Before
self.m_stage_module_map = {
    "sam": ["sam_model"],
    "clip": ["vision_model"],
    ...
}

# After
self.m_stage_module_map = {
    "sam": ["model.sam_model"],
    "clip": ["model.vision_model"],
    "projector": ["model.projector"],
    "prefill": ["model.embed_tokens", "model.layers", "model.norm"],
    "decode": ["lm_head"],
}
```

### Fix 3: ModelWrapper for Full Model Tracing

Created wrapper to convert kwargs to positional args for fvcore:

```python
class ModelWrapper(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, input_ids, images, images_seq_mask, images_spatial_crop):
        return self.model(
            input_ids=input_ids,
            images=images,
            images_seq_mask=images_seq_mask,
            images_spatial_crop=images_spatial_crop,
        )

wrapper = ModelWrapper(self.m_session.m_model)
wrapper_inputs = (
    input_ids,
    model_kwargs["images"],
    model_kwargs["images_seq_mask"],
    model_kwargs["images_spatial_crop"],
)

flops_full = FlopCountAnalysis(wrapper, wrapper_inputs)
```

Also added module name fixing to strip "model.model." prefix from wrapper.

### Fix 4: Real SAM Features for CLIP

Updated CLIP analysis to use real SAM features from global view:

```python
# Get real SAM features from GLOBAL VIEW (not crops)
sam_model = getattr(base_model, "sam_model", None)
_, model_kwargs = self.prepare_inputs(config)
images_ori = model_kwargs["images"][0][1]  # Global view: [1, 3, H, W]

if sam_model is not None:
    with torch.no_grad():
        sam_features = sam_model(images_ori)  # batch=1
else:
    # Mock features as fallback
    sam_features = torch.randn(1, 64, 64, 1024, ...)

# Now trace CLIP with correct inputs
flops_clip = FlopCountAnalysis(vision_model, (images_ori, sam_features))
```

---

## Final Results

With all fixes applied, the static analysis now correctly reports:

| Stage | Parameters | FLOPs | Notes |
|-------|-----------|-------|-------|
| **SAM** | 95.57M | 571.92G | Isolated fvcore analysis on crops |
| **CLIP** | 303.18M | 77.68G | Isolated fvcore analysis with real SAM features |
| **Projector** | 2.62M | 10.74G | Isolated fvcore analysis |
| **Prefill** | 454.49M | 487.23G | Analytical estimate |
| **Decode** | 165.48M | 0.49G | Analytical per-token estimate |
| **TOTAL** | **1021.34M** | **1148.05G** | Full model trace |

### Verification

All vision stages now show:
- âœ… Non-zero parameters
- âœ… Non-zero FLOPs (extracted and isolated match)
- âœ… Operator breakdowns (linear, conv, layer_norm, etc.)
- âœ… Activation counts

---

## Usage

Run the static analysis script:

```bash
# Default (cuda:1, standard config)
pixi run python scripts/deepseek-ocr-static-analysis.py

# Custom device and configuration
pixi run python scripts/deepseek-ocr-static-analysis.py \
  --device cuda:0 \
  --base-size 1024 \
  --image-size 640 \
  --seq-len 2048 \
  --output tmp/my-analysis
```

Outputs are generated in the specified directory:
- `static_compute.json` - Machine-readable report
- `static_compute.md` - Human-readable markdown report

---

## Technical Details

### Model Structure

```
DeepseekOCRForCausalLM
â”œâ”€â”€ model (DeepseekOCRModel)
â”‚   â”œâ”€â”€ sam_model (ImageEncoderViT) - 95.57M params
â”‚   â”œâ”€â”€ vision_model (VitModel) - 303.18M params
â”‚   â”œâ”€â”€ projector (MlpProjector) - 2.62M params
â”‚   â”œâ”€â”€ embed_tokens (Embedding)
â”‚   â”œâ”€â”€ layers (ModuleList) - 12 layers
â”‚   â””â”€â”€ norm (RMSNorm)
â””â”€â”€ lm_head (Linear) - 165.48M params
```

### Data Flow for Analysis

1. **SAM**: Takes crops [4, 3, 640, 640] â†’ outputs [4, 64, 64, 1024]
2. **CLIP**: Takes global view [1, 3, 1024, 1024] + SAM features [1, 64, 64, 1024] â†’ outputs features
3. **Projector**: Takes vision features [1, num_tokens, 2048] â†’ outputs [1, num_tokens, 1280]
4. **Prefill**: Takes full sequence with vision tokens â†’ analytical FLOPs
5. **Decode**: Per-token generation â†’ analytical FLOPs per token

### Key fvcore Limitations

1. **JIT Tracing**: Cannot trace through complex control flow
2. **Kwargs Support**: Only positional arguments work reliably
3. **Custom Ops**: May need operator handlers for non-standard operations
4. **Shape Dependence**: FLOPs vary with input dimensions

---

## Files Modified

1. **`src/llm_perf_opt/runners/dsocr_analyzer.py`**
   - Fixed module access paths in `_analyze_sam_isolated()`
   - Fixed module access paths in `_analyze_clip_isolated()`
   - Fixed module access paths in `_analyze_projector_isolated()`
   - Updated `m_stage_module_map` with correct prefixes
   - Added ModelWrapper in `analyze_full_model()`
   - Fixed module name extraction logic
   - Updated CLIP to use real SAM features from global view

2. **`scripts/deepseek-ocr-static-analysis.py`** (created)
   - Reusable CLI script for running static analysis
   - Configurable device, dimensions, output directory

3. **`scripts/inspect_model_structure.py`** (created)
   - Diagnostic script to inspect model structure

---

## Next Steps

With vision stages now properly analyzed, the next steps are:

1. **T103**: Integrate into `llm_profile_runner.py`
   - Call analyzer before repeated profiling runs
   - Store results for MFU computation

2. **T104**: Add Hydra configuration
   - Create `conf/static_analysis/default.yaml`

3. **T105**: Enhance MFU computation
   - Use static FLOPs from this report instead of pure analytical

4. **T106**: Testing & validation
   - Verify FLOP counts against vendor benchmarks
   - Compare isolated vs extracted results

---

## Success Metrics

âœ… All vision stages report non-zero FLOPs
âœ… Isolated and extracted FLOPs match (where applicable)
âœ… Parameter counts sum to ~1.02B (correct model size)
âœ… Operator breakdowns show expected ops
âœ… Graceful error handling throughout
âœ… Production-ready reusable script

---

**Status**: All vision stage FLOP computation issues resolved! ðŸŽ‰
