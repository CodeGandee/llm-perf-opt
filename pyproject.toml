[project]
authors = [{name = "igamenovoer", email = "igamenovoer@xx.com"}]
dependencies = []
name = "llm-perf-opt"
requires-python = ">= 3.11"
version = "0.1.0"

[build-system]
build-backend = "hatchling.build"
requires = ["hatchling"]

[tool.pixi.workspace]
channels = ["conda-forge"]
platforms = ["linux-64"]

[tool.pixi.pypi-dependencies]
llm_perf_opt = { path = ".", editable = true }
pillow = "*"
torch = { version = ">=2.5.1", index = "https://download.pytorch.org/whl/cu124" }
torchvision = { version = ">=0.20.1", index = "https://download.pytorch.org/whl/cu124" }
triton-kernels = { git = "https://github.com/triton-lang/triton.git", rev = "v3.5.0", subdirectory = "python/triton_kernels" }

[tool.pixi.dependencies]
# Ensure a Python version compatible with current PyTorch wheels
python = ">=3.11,<3.13"

[tool.pixi.system-requirements]
cuda = "12.0"

[tool.pixi.pypi-options]
# Prefer vLLM nightly wheels for latest features
extra-index-urls = ["https://wheels.vllm.ai/nightly"]

[tool.pixi.tasks]
install-vllm-nightly = { cmd = "bash scripts/install-vllm-nightly.sh" }
verify-vllm = { cmd = "python - <<'PY'\nimport vllm, torch, PIL; print('vllm', vllm.__version__); print('torch', torch.__version__); print('pillow', PIL.__version__)\nPY" }
