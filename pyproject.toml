[project]
authors = [{name = "igamenovoer", email = "igamenovoer@xx.com"}]
dependencies = ["transformers==4.46.3", "tokenizers==0.20.3", "einops>=0.8.1,<0.9", "addict>=2.4.0,<3", "easydict>=1.13,<2", "mypy>=1.18.2,<2", "ruff>=0.14.2,<0.15", "nvtx>=0.2.13,<0.3"]
name = "llm-perf-opt"
requires-python = ">= 3.11"
version = "0.1.0"

[build-system]
build-backend = "hatchling.build"
requires = ["hatchling"]

[tool.pixi.workspace]
channels = ["conda-forge"]
platforms = ["linux-64"]

[tool.pixi.pypi-dependencies]
llm_perf_opt = { path = ".", editable = true }
pillow = "*"
torch = { version = ">=2.5.1", index = "https://download.pytorch.org/whl/cu124" }
torchvision = { version = ">=0.20.1", index = "https://download.pytorch.org/whl/cu124" }
triton-kernels = { git = "https://github.com/triton-lang/triton.git", rev = "v3.5.0", subdirectory = "python/triton_kernels" }
flash-attn = "==2.7.4.post1"

[tool.pixi.dependencies]
# Ensure a Python version compatible with current PyTorch wheels
python = ">=3.11,<3.13"
ninja = "*"
nsight-compute = ">=2025.2.1.3,<2026"

[tool.pixi.system-requirements]
cuda = "12.0"

[tool.pixi.pypi-options]
# Prefer vLLM nightly wheels for latest features
extra-index-urls = ["https://wheels.vllm.ai/nightly"]
no-build-isolation = ["flash-attn"]

[tool.pixi.tasks]
install-vllm-nightly = { cmd = "bash scripts/install-vllm-nightly.sh" }
verify-vllm = { cmd = "python - <<'PY'\nimport vllm, torch, PIL; print('vllm', vllm.__version__); print('torch', torch.__version__); print('pillow', PIL.__version__)\nPY" }
